{"summary":"The authors present a solution to automating web tasks such as checking on shopping orders. The solution leverages LLMs that break down the task into subtasks, executes those subtasks in the browser, verifies the subgoals are accomplished, can self corrects and replan if necessary, and leverages in context examples retrieved from an exemplar bank created by the authors. Experimental results show the authors' superior approach compared to the literature on WebArena, a popular benchmark in the literature.","soundness":"3: good","presentation":"3: good","contribution":"3: good","strengths":"Summary:\n\nSolves a relevant problem\nAdopts a solution that is based on the latest technology\nBeats the state of the art with their experimental results on a well-known benchmark from the literature\nDetails: The problem of automating web tasks is difficult and very relevant in this age of enterprise productivity. Many tasks are quite repetitive and could benefit from automation but the diversity of browsers and apps and tasks makes it challenging for automated systems.\n\nLLMs have proven beneficial and the paper not only leverages them but also tests GPT-4o which is one of the newest and less costly models compared to others from the literature.\n\nThe proposed framework introduces three key components to the LLM pipeline: 1) sub-goal verification, 2) self-correction and 3) exemplar bank. Each of these components are not particularly original but combining them into a single framework and applying this framework to the web task automation leads to state of the art of results.","weaknesses":"Summary:\n\nLimited experimental results and analysis including missing computational cost analysis, error analysis especially when linked to the various contributed components in their framework\nTyping and grammar mistakes\nDetails: The experimental results show that the proposed approach (including individual components) do improve the state of the art on the web arena benchmark. The authors compare to other approaches from the literature and do an ablation study on the components they proposed. However, the experimental analysis is still missing some key results that could help the community understand and evaluate this approach better. Notable, the authors perform multiple LLM calls during their pipeline. Quantifying the computational cost (whether with number of calls per input or some other metrics) would help evaluate the approach and compare to other in the literature. Furthermore, the authors do not analyze what errors benefited more from what components in their pipelines. What types of errors needed replanning, which were addressed with reflection only, why did some of the verifications fail, etc. Finally, the authors perform an end to end evaluation but do not evaluate each component individually on intrinsic metrics; e.g., how often was the reflection component able to correct an error that is within its scope, etc.","questions":"NA","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"6: marginally above the acceptance threshold","confidence":"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","code_of_conduct":true}