Summary:
The paper introduces EXE, a new benchmark designed to evaluate language models (LLMs) on their ability to execute Python code sourced from real-world applications. This benchmark aims to address several limitations of existing evaluations, particularly the issues of scalability, task diversity, training data contamination, and benchmarking costs.

The benchmark comprises over 30,000 tasks drawn from 1,000 popular GitHub repositories, spanning different complexities and computational operations like logical inference, mathematical reasoning, and state management.

To construct this benchmark, the authors first select the top 1,000 most popular pypi packages and collate the corresponding github repos, after that, the authores perform a static ast analysis to filter to functions with LLM generatable argument and return type annotations. Finally, the authors apply LLM to generate test cases.

The evaluation with GPT-4 model demonstrate the limitation of existing code models.

Soundness: 1: poor
Presentation: 3: good
Contribution: 2: fair
Strengths:
1. This paper is well-written and easy to follow.
2. Benchmarking code LLM is an important problem.
3. The findings are interesting.
Weaknesses:
1. The motivation for this work is not clearly articulated.
The paper proposes benchmarking the code execution capabilities of LLMs, but it is unclear why such a capability is needed given the existing roles of compilers and interpreters. A possible motivation might be that LLMs are more lightweight and could predict execution outcomes without running the code. However, I did not see any evaluation results to support this assumption.

2. The paper suggests that the proposed dataset can guard against data contamination [1, 2], but lacks a detailed explanation of how this is achieved.
The authors claim that the dataset is dynamically collected from GitHub, which could help mitigate contamination. However, since the benchmark is built from popular GitHub repositories that do not frequently change, the dataset may not be as dynamic as implied. Additionally, because the test inputs are generated by LLMs, it is unclear how this setup effectively prevents data contamination.

3. Certain methodological details are missing.
First, in "Function Selection and Dependency Collation," the authors mention using static AST analysis, but it is not clear how this process is performed. Second, regarding the error metric, the authors state that they "compare the type and message (excluding stacktrace) using a language model comparison," which is described too vaguely to understand how this metric is actually computed.

4. This work lacks soundness in the following areas:
(1) The authors claim the benchmark is diverse; however, there is no diversity evaluation regarding the prompts and solutions. (2) Since all test cases are generated by an LLM, there is no guarantee that the test cases are sound or appropriate for the programs. Given that some test cases result in errors during execution, this raises soundness concerns.

5. Minor: Some figures are of low resolution and unclear.
[1] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models

[2] PPM: Automated Generation of Diverse Programming Problems for Benchmarking Code Generation Models

Questions:
Why we need to benchmark LLM's executation capability.

Can you introduce more details of the approach and the evaluation?

Flag For Ethics Review: No ethics review needed.
Rating: 3: reject, not good enough
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes