Summary:
This paper introduces a new benchmark EXE, focusing on testing the capability of LLMs to simulate code execution. EXE is made up of over 30000 tasks derived from 1,000 popular Python repositories on GitHub. In this scenario, LLMs need to execute code, involving operations like mathematical reasoning, logical inference, loop execution, and maintaining internal variable states. This paper provides a shallow breakdown on this. The pipeline to create EXE involves selecting and preprocessing GitHub repositories, synthesizing inputs based on function signatures, and then creating test cases (unit tests, and potentially, chaining functions tests) with the inputs. The authors claim their pipeline is automatic and capable of continuous new task generation with newest repositories to avoid test set contamination.

————after rebuttal————

The substantial revisions made during the rebuttal period addressed some of my concerns regarding model evaluation and test set contamination. The clarifications on dependency solving are also convincing. However, as the rest of my concerns are not fully addressed and the substantial revisions are making it a new paper with too many raw details without careful organizations. I think it would be better to be revised and submitted to follow-up venues like ICML. Besides, knowing the capabilities of LLMs in code executions should be the outcome of this paper, and I think current easy subset(see weakness 8) evaluation somehow weakens it.

Nonetheless, I have raised my score to 5 and presentation to 2 to praise for the authors’ efforts.

Good luck,

Reviewer o62o

Soundness: 2: fair
Presentation: 2: fair
Contribution: 2: fair
Strengths:
1.Provide a benchmark of real-world Python code for testing LLM execution, the test cases are significantly harder and more representative for real-world usage, therefore providing a more realistic assessment of model capabilities,

2.Establish an automatic pipeline to create a real-world dataset for LLM-based code execution tasks.

3.Cover a wide range of programming concepts and can be potentially scaled up or updated with new tasks.

4.The unit-test based evaluation is correct, the authors also mention the potential to create more complicated test cases like using chaining functions.

Weaknesses:
Major weaknesses:
1.Only GPT-4o and GPT-4o-mini are evaluated, contrary to the claim of evaluating "several state-of-the-art LLMs." Additional evaluation with different LLMs are recommended, like Claude, Gemini, Deepseek, Phi, Qwen, etc.

2.The claim of "avoiding training on the test set" relies heavily on the quality and effectiveness of the pipeline's ability to generate new test cases, which is not thoroughly demonstrated in the paper, no supplementary materials provided either. The Lack of supportive materials (either the benchmark itself or its creating code) to support claims about the framework's capabilities, weakens the contribution of a dataset paper.

3.The handling of import dependencies and the process of inlining required elements are not clearly explained. It's technically important here. Need clarification.

4.A bit limited to Python code, which may not represent the full spectrum of programming challenges across different languages. Since LLMs are pretrained on various programming languages, it's worth to know the execution capability on other programming languages.

5.Poor quality of figures in the paper, with low-precision images that are difficult to see clearly, the authors should use vector figures instead of jpgs or pngs,

6.The appendix uses 8 pages to show an example, which is excessive and poorly organized, besides, it's still not intuitive for understanding. This needs significant revision for clarity and conciseness.

Minor weaknesses:
7.A bit limited evaluation metrics, using only Pass@1 accuracy. Considering more evaluations on Pass@k, or try some self-correction mechanism with LLM.

8.Filtering on limited acceptable types and functions seems to make EXE an easy subset of the real real-world programs, although it is a fair design choice for a benchmark to avoid environment configuration issues. I think it's more interesting to know the capabilities and limitations of LLMs when executing harder cases, containing real-world types like numpy.array, torch.tensor for example. Can the authors add some discussions about their findings here?

Typos and Presentation Issues:
Line 294: tense issues, ...increase task difficulty, however bit manipulation and boolean operations only showed... Should use unified tense throughout a paragraph.

Line 297: however for loops on (73 Pass@1) on average did not have a significant impact.

Line 303: Incorrect spacing on the title of the rightmost subfigure.

Figure 7: Examining only on LLM really executed code makes the accuracy normal now. However, it seems the results are not clearly illsutrated (only a small part of the figure is valid now, which is not clear). Consider to use some new figures.

Appendix A.2: These are important part of your paper, since current version only uses 8 pages, consider to move this section to the main page and explain them with more details.

Questions:
1.In the appendix, could you clearly differentiate between original code, imported dependencies, and LLM execution steps? Can you show the full LLM output and indicate at which steps they fail?

2.How does EXE compare to existing code execution benchmarks in terms of task diversity and difficulty when "executing code"?

3.Can you elaborate on the measures taken to ensure the generated test cases are meaningful, diverse, and correctly assess code execution abilities?

4.How do you validate that the newly generated test cases are indeed novel and not present in existing LLM training sets?

5.Has the chaining-function been implemented now? Because i think it will be of more interests to the community if EXE can create more complicated test cases automatically.

Flag For Ethics Review: No ethics review needed.
Rating: 5: marginally below the acceptance threshold
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes