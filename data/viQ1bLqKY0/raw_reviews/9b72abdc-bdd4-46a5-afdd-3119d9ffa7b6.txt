Summary:
The authors introduce a dataset of executable python functions mined from Github. The functions chosen have certain type annotations for which test cases can be generated. The task consists in providing a code snippet as well as the input arguments into an LLM and asking the LLM to predict the output (this task has been referred to as "program induction" in some literature of the past, and I will refer to it as "program understanding")

The authors argue that this is a non-trivial benchmark and that the methodology allows the benchmark to evolve over time to include test cases or functions that are not in the training set. The authors also argue that this program understanding task could be an useful gauge of LLMs performance for coding tasks.

The authors evaluate GPT4o and GPT4o-mini on this task and provide some analysis on performance by certain proxies for ``difficulty" such as lines of code, number of function calls, etc.

Soundness: 2: fair
Presentation: 2: fair
Contribution: 2: fair
Strengths:
The authors deserve credit for their creative use of open-source software on Github. I believe that more executable coding benchmarks will be beneficial to the community and the authors have elements to create something very interesting! The steps taken to create the dataset seem non-trivial and the scale of the dataset is notable (>30K functions). There is preliminary evidence that the task is non-trivial, and the authors also have interesting analysis on factors that lead to more difficult program understanding on this task. I think there is potential for the authors to leverage their ingenuity in constructing this dataset for interesting applications. After skimming CruxEval which seems to propose a similar approach, my judgment is that the underlying dataset scale and difficulty of EXE is more noteworthy.

Weaknesses:
I think the motivation of this paper is great, and the creativity to create an executable programming benchmark is excellent! I think there is great potential in this work! I would recommend the authors try to focus on some of the following facets.

Clarification of Test case generation methodology
I may have missed it, but I tried to look for details on methodology of test case input generation. The authors are clear on the accepted types are allowed for inputs/outpus, but it is unclear how generation is done. The best I could find is: "Based on the type definition (used for setting the function calling schema) inputs/ output pairs have been generated with the goal of maximising diversity of control flow paths within the function." and "Using the argument type annotations we construct a LLM function calling schema that generates a diverse set of inputs." The paper requires more details and clarification on this, and depending on the methodology chosen, this could affect the merits of the approach.

Experiments / Lack of Models Considered
Because this is a datasets and benchmarks paper and the paper's motivation emphasizes "difficulty" of the task, not enough is done to substantiate this claim. My expectation for a dataset/benchmark paper should be at least to evaluate numerous open source models (e.g. CodeLLama, LLama3 family, CodeT5, etc) of varying sizes in addition to commercial models. Additionally, only 2 commercial models from OpenAI are used. Performing wider evaluation will strengthen these claims and the analysis, otherwise, it is an open question on how other models would perform on this task.

The framing of experiments + context of other works (a potential lack of novelty)
The authors do not distinguish their approach or experiments from a dataset like CodeNet. The code understanding experiments provided here can also be done with CodeNet. If the authors could show that LLM performance or the nature of LLM performance is different on their task vs. CodeNet, this would substantiate the contribution. Of course the code on github is more diverse in nature, but on the other hand, the input/output types are still limited, and a dataset like CodeNet is multi-lingual.

My recommendation would be to consider other creative uses of this dataset besides the ones you currently have.

Polished Writing
A paper for this venue should have a higher standard of polishing. For example, the term AST should be introduced as an Abstract Syntax Tree (AST) and referred to as AST. At one point the authors colloquially refer to evaluation benchmarks as "evals." These are minor points and easy to fix, but are nevertheless are standards.

Clarification on Licensing, Copyright, etc.
I did not see clarification if the authors filtered code for permissively licensed software and if the dataset falls under acceptable use of the software.

Questions:
Can you provide more detail on the methodology of test case input generation? Even the code used for this will work, although an explanation would help as well.

Can you provide more explicit clarification on your proposed contributions, especially in context of a dataset like CodeNet. Besides the fact that the functions are from Github and that the dataset in theory can evolve, is there anything else I have misunderstood?

[Recommendation to address limitation] Are you able to provide more comprehensive evaluations of other model? If the authors have access to computing resources, I strongly recommend open access models like CodeLlama to avoid API costs. If the authors have access to API credits, I would recommend at least one very large commercial model such as sonnet 3.5 or Llama3.1 405 Instruct (e.g. hosted on AWS bedrock). Although alone, I do not think these will convince me the paper should be accepted.

Licensing / Copyright: Can you explain what licenses exist for the data mined for the benchmark? e.g. was filtering done for permissive licenses? Additionally if more context can be provided then if the dataset is a fair and acceptable use of the software under consideration.

5.Clarification on Side-Effects, Determinism, and Execution Environment: Can you explain how you implement ensuring that there are "no side-effects" and that determinism indeed holds? I understand there are some banned imports, but can you provide more clarification? How do we know that this is indeed comprehensive enough to make these claims? Additionally, can you specify the python version / environment used for executing the python code? In a perfect world, it would be good to have a docker container with the same environment used to execute these programs so that the input/output examples are indeed reproducible.

Flag For Ethics Review: No ethics review needed.
Rating: 3: reject, not good enough
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes