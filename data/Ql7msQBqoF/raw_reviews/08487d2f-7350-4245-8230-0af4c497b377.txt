Summary:
This paper applies ideas from prompt optimization to optimizing KBs for RAG. The domain considered is text-to-code using documentation as context, focusing on lower-resource coding languages. The authors take a multi-agent approach, splitting a KB into multiple documents and proposing edits for each document according to a fixed set of edits, based on execution feedback. After a global document selection stage, each document is edited by a separate editing agent (implemented via REACT prompting and PromptAgent's Monte-Carlo tree search) that proposes a sequence of edits to the document. Their method, MAC-CAFE, is evaluated on five text-to-code datasets, with two covering cases where the documentation is incomplete and three covering cases where the documentation is incorrect. The method is evaluated by looking at accuracy before and after editing, as well as coherence metrics. MAC-CAFE improves accuracy on the test set for all datasets as compared to the base model and a PromptAgent-based baseline. MAC-CAFE also results in more coherent documentation according to G-Eval.

Soundness: 1: poor
Presentation: 1: poor
Contribution: 2: fair
Strengths:
Motivating problem: The problem of outdated KBs (especially in text-to-code) is interesting and well-motivated, as information in these domains does change consistently. A solution for automatically keeping documentation up-to-date here would indeed be interesting.

Multiple domains: The authors evaluate the method across multiple domains and datasets and include two different settings (incomplete vs. incorrect info). Their method shows improvements across all domains.

Clear method figure: Figure 2 is easy to parse and describes the MAC-CAFE method in a way that is easy to understand.

Weaknesses:
Limited methodological contribution: Besides the splitting of the problem into multiple documents, the paper seems like a fairly direct application of PromptAgent with limited technical novelty besides the application to a new domain.

Assumption of error knowledge: The method uses LLMs to generate code and then use feedback from generated code to update the docs. In lines 194-199, the authors correctly point out that there could be multiple sources of error, including sources that do not stem from incorrect docs. However, the authors then say that they assume errors result from only from incorrect docs. It's not clear at all from the writing how -- or whether -- this is enforced, i.e. how the authors ensure that the errors are in fact from errors in the docs rather than the generator's shortcomings. This is especially troubling given that they evaluate on lower-resource coding languages where the model might be worse at generating even with correct docs. If this assumption is enforced, the authors should explain how. If it's not enforced, it's the authors responsibility to convince readers that their benefits come from the system in fact improving the docs in some interpretable way, as opposed to addressing simpler kinds of errors.

Limited results: the "extensive experiments" mentioned in fact boil down to 2 short paragraphs on the last page of the paper. There are no ablations for the design choices made (e.g. action space, state representation) and no ablations showing the necessity of splitting the task into a multi-actor setup. The authors only evaluate a single LLM. There is no analysis of the resulting KBs after editing.

Unclear baselines and metrics: It's not clear what the baseline PromptAgent-E is/how it is implemented, why it was chosen, and why it is a fair/relevant baseline. This kind of detail should be brought up in the main paper (which the paper fails to do) and can then be elaborated in supplementary material (which the paper lacks). The authors also do not convincingly argue for the completeness metric measures completeness. The coherence metric is not clearly explained. The metric section is split across 5 and 6.3 in a way that is very hard to follow

Unclear writing and organization: In addition to the clarity issues above, the rest of the paper also omits a large number of details. To give a few examples:

The method hinges almost entirely on prompting but the authors do not provide their prompts.
In Table 3, it's not clear what any of the numbers refer to. They aren't percentages, but they also don't add up to the number of documents.
Much of the writing could be compressed. There are several sections that conceptually should be subsections/compressed together. At the same time, other parts are overcompressed, e.g. the results section, where Table 3 is unreadably small.
Method described as using gradients: The method (which is in fact gradient-free) is described in terms of gradients. My feeling is that the authors should make clear that the "gradient" part here is purely a helpful metaphor, as the method does not actually compute any gradients at all. However, from the writing, this point is not brought through clearly.
Questions:
One suggestion for measuring effectiveness: you could measure how well MAC-CAFE recovers the existing documentation after a version change. After a major version update (ideally after an LLM's cutoff date) you could give the model the new library along with the old documentation and measure how well you rediscover the new (human-generated gold) docs.

Flag For Ethics Review: No ethics review needed.
Rating: 1: strong reject
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes