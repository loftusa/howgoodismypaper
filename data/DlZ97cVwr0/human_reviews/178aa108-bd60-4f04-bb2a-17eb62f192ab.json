{"summary":"This paper introduces a benchmark for evaluating the recall of language models in the domain of small organic molecules. Specifically, based on the famous dataset GDB-13, the authors prepare a new dataset with four subsets, e.g., a new subset contains molecules that share a certain percentage of substructures with aspirin. Based on the constructed dataset, the molecule generation capability of language models (LMs) in terms of recall before and after fine-tuning has been evaluated. A new method for predicting the recall of LMs has also been designed. The average probability of a desired molecule to be generated and the ground truth recall values are used to build a regression model for the recall prediction. The evaluation demonstrated the correlation is more than 0.99. Finally, a recall-oriented molecule generation method and a loss function have been introduced to boost the recall of LMs.","soundness":"2: fair","presentation":"3: good","contribution":"2: fair","strengths":"An interesting and important problem in analyzing the recall of language models.\nMultiple solutions with promising results have been proposed in the same work\nThe paper is well-written","weaknesses":"Even though the motivation is clear and good, the studied objective does not fit the motivation well, is the recall metric more important in the molecule generation domain?\nMany design choices are unclear, e.g., why use Beam search in section 3.4 not others?\nMany problems, e.g., capability estimation and new loss design, have been studied, but each of them lacks a comparison with baselines.\nOverall, this paper studies an important problem and proposes promising solutions for recall estimation and LMs enhancement. However, there are some concerns that need to be addressed.\n\nFirstly, even though the main point, evaluating whether a model can generate all correct outputs is important for safety-critical problems, it is unclear whether this is the case for the studied objective molecule generation. It is better to give clear motivation for the importance of evaluating recall for this task.\n\nFor the subset construction, in Table 1, it is unclear how the threshold is determined, e.g., 0.4 for Sasp and 0.2 ≤ sim(m, d) ≤ 0.2165. Please clarify it.\n\nIn Section 4.1, Table 2 and Table 3 suggest different solutions as the best, which one we should accept in practice. It is better to add more discussion here.\n\nIn Section 4.2, considering the recall estimation, there are many works that have been proposed to evaluate deep learning models in an unsupervised manner [1, 2, 3], it is necessary to at least discuss the difference between the proposed method and these works.\n\nIn Section 4.3, it is unclear why Beam search is used here since there are many other options (search methods).\n\nIn Section 4.4, first, it is better to add baselines without using the designed loss function in Table 5. Besides, the recall values decreased after comparing the results in Table 5 and Table 4. It is unclear which factors lead to this degradation.\n\n[1] Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. [2] Estimating Model Performance Under Covariate Shift Without Labels. [3] Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift","questions":"Please check my comments above.","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"5: marginally below the acceptance threshold","confidence":"3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.","code_of_conduct":true}