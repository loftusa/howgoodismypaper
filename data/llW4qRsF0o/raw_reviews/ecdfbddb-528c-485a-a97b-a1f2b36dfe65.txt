Summary:
This paper proposes a framework called Physics Transfer Learning (PT) that aims to learn underlying physics from low-fidelity data to enable extrapolation to high-fidelity data. The authors conduct experiments on two entirely different domains: crystal and brain morphologies. While the paper claims to learn the underlying physics through the PT framework, it does not propose any specific method for achieving this beyond simply inputting data and training a model via supervised learning. Furthermore, there is a significant lack of consideration for competing methods and related work in AI, which raises substantial concerns about the contribution and novelty of the work.

Soundness: 1: poor
Presentation: 2: fair
Contribution: 1: poor
Strengths:
The idea of considering ellipsoids in brain morphology analysis is intriguing. With more extensive analysis and theoretical development, this concept has the potential for significant advancement in the field.
Weaknesses:
Despite the title "Physics Transfer," the proposed framework does not modify the input data format used in existing Machine Learning Force Field (MLFF) methods or models that embed brain networks. No additional optimization techniques are introduced. The framework does not adequately consider physics principles or employ transfer learning methodologies.

The paper lacks any discussion of AI methodologies or competing methods. The absence of comparisons with existing approaches makes it difficult to evaluate the effectiveness and innovation of the proposed framework.

There is no illustration or explanation of how the "physics" is learned within the model. The paper fails to demonstrate the underlying mechanisms that enable the model to capture or learn physical laws.

The structure and composition of the proposed framework remain unclear. The paper does not present a cohesive framework that can be commonly applied across two entirely different domains, such as crystal structures and brain morphologies.

Questions:
Model References: Which models did you reference or build upon for your experiments? Specifically, what is the CNN-based crystal model you used, and what Graph Neural Network (GNN) methods were applied? These details are not provided in the paper, and including them would help in understanding and replicating your work.

Learning Physics: You claim that your framework learns physics, but how is this achieved? In the context of Physics-Informed Neural Networks (PINNs), learning physics involves incorporating underlying equations to solve partial differential equations (PDEs). Does your approach relate to methods like "PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks" (ICLR 2024, Zhao et al.)?

Related Work: Why did you not include recent relevant papers in your results? In the MLFF field, numerous models like SchNet, DimeNet, Allegro, and Equiformer are well-established. Methods utilizing crystal lattice parameters, such as Matformer, PotNet, Crystalformer, and Comformer, were also not considered. How does your work compare to these existing methods, and why were they omitted from your analysis?

Domain Combination: What is the rationale behind combining two domains with no apparent commonality, such as crystals and brain morphologies? How does this contribute to the generality or applicability of your proposed framework? A clear explanation would help in understanding the motivation and potential benefits.

Clarification of Figure 1(a): Could you clarify the content and purpose of Figure 1(a)? The caption reads: "Machine learning, constrained by data density and coverage, serves as a potent complement to traditional theories for interpolating and extrapolating solutions, especially as data quality and quantity increase." However, it's unclear how this statement is represented in the figure. Additionally, the text refers to Figure 1(a) as illustrating recent advancements in ML and AI as a data-driven alternative. A more detailed explanation would enhance the reader's comprehension of your conceptual framework.

Brain Morphology Representation: The shape of the brain is highly complex and significantly different from simple spheres or ellipsoids. Can using such simplified geometric shapes genuinely benefit the model in understanding brain morphology? Handling data from different domains is sensitive and typically requires careful adaptation or alignment processes. How do you justify that combining these datasets without such processes would be beneficial to the model's performance?

Transfer Learning Techniques: The term "Physics Transfer" suggests the use of transfer learning methodologies. What specific transfer learning methods did you employ in your framework? Detailing these techniques would strengthen your contribution and clarify how physics is transferred between domains.

Flag For Ethics Review: No ethics review needed.
Rating: 1: strong reject
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes