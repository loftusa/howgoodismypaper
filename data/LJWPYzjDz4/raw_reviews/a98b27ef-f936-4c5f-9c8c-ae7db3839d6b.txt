Summary:
This paper proposes a new downsampling strategy for image compression on edge devices. The method achieves non-uniform sampling by erasing and squeezing the image blocks. In addition, the down-sampled image can be recovered using a transformer. Experiments show that the proposed method achieves superior rate-distortion performance (distortion measured by Brisque, Pi and Tres) with reduced computational complexity.

Soundness: 2: fair
Presentation: 3: good
Contribution: 2: fair
Strengths:
The paper is well written and easy to follow.
The idea of erasing and squeezing is interesting.
Weaknesses:
The proposed method is not suitable for image compression. The erasing process removes the image blocks randomly, which can lead to irretrievable losses of objects. An importance-based or smoothness-based method may be a better choice.
There is no comparison with uniform sampling methods, which is necessary to show the advantages and disadvantages of the proposed sampling method.
The non-reference distortion metrics used in this paper are not appropriate. As far as I know, the non-reference metrics can only measure whether the image is real, which has nothing to do with fidelity. So we can generate a completely different image at the decoder size with an image generation model without any bitstreams to get a high rate-distortion performance, which may be meaningless in applications.
Questions:
How does the proposed method perform when the distortion is measured using reference metrics such as PSNR and MS-SSIM?
How does the proposed method compare to uniform sampling methods?
Are there specific scenarios where we need to measure the distortion with non-reference metrics? Especially for image compression?
Flag For Ethics Review: No ethics review needed.
Rating: 3: reject, not good enough
Confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.
Code Of Conduct: Yes