{"summary":"This paper proposes a new downsampling strategy for image compression on edge devices. The method achieves non-uniform sampling by erasing and squeezing the image blocks. In addition, the down-sampled image can be recovered using a transformer. Experiments show that the proposed method achieves superior rate-distortion performance (distortion measured by Brisque, Pi and Tres) with reduced computational complexity.","soundness":"2: fair","presentation":"3: good","contribution":"2: fair","strengths":"The paper is well written and easy to follow.\nThe idea of erasing and squeezing is interesting.","weaknesses":"The proposed method is not suitable for image compression. The erasing process removes the image blocks randomly, which can lead to irretrievable losses of objects. An importance-based or smoothness-based method may be a better choice.\nThere is no comparison with uniform sampling methods, which is necessary to show the advantages and disadvantages of the proposed sampling method.\nThe non-reference distortion metrics used in this paper are not appropriate. As far as I know, the non-reference metrics can only measure whether the image is real, which has nothing to do with fidelity. So we can generate a completely different image at the decoder size with an image generation model without any bitstreams to get a high rate-distortion performance, which may be meaningless in applications.","questions":"How does the proposed method perform when the distortion is measured using reference metrics such as PSNR and MS-SSIM?\nHow does the proposed method compare to uniform sampling methods?\nAre there specific scenarios where we need to measure the distortion with non-reference metrics? Especially for image compression?","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"3: reject, not good enough","confidence":"5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.","code_of_conduct":true}