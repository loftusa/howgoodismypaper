Summary:
The manuscript presents an original and challenging dataset for mathematical problem-solving, encompassing various subjects and difficulty levels. Then the paper conducts comprehensive examinations on both open-source and closed-source LLMs. The findings reveal that while closed-source models currently lead, open-source models are rapidly catching up, highlighting the competitive landscape of LLM capabilities in mathematical problem-solving.

Soundness: 2: fair
Presentation: 2: fair
Contribution: 2: fair
Strengths:
The paper is well-motivated, as GPT-4o poses a significant challenge to current mathematical benchmarks. The introduction and open-sourcing of high-quality, difficult mathematical problems is a meaningful contribution to the field.

The dataset features distinct levels of difficulty and sub-domain classifications, which enhance its uniqueness.

Weaknesses:
The manuscript lacks coverage of important related work and further clarification on the difference and improvements compared to them.

OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems
Omni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models
OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models
There are missing baseline comparisons that are crucial for evaluating the open-source models on the proposed dataset, like Qwen2.5-MATH, DeepSeek-Coder, and so on.

It is unclear how the authors ensure that the data has not been previously encountered. If the problems are original, details regarding the creation principles and methodologies should be included in the paper. Additionally, how is the correctness of answers verified? Have the authors conducted cross-validation or sampling tests to ensure reliability? What is the accuracy rate?

The conclusions drawn seem predictable and do not provide substantial insights. Are there fine-grained analyses and interesting findings?

Questions:
While the authors mention diverse answer types, there are only three categories presented. Moreover, there seems to be a discrepancy between the data in Figure 2 and that described in line 304. Given that existing mathematical models have undergone extensive pre-training and can effectively comprehend natural language queries, why does the diversity of answer types pose a greater challenge for these models? Is there a variance in accuracy across different question types? Do models exhibit inconsistencies when dealing with various types of questions?
The creation details of the problems:
Please include principles and details of problem creation in an appendix. How is answer correctness ensured? Have cross-validation or sampling methods been employed? What is the accuracy rate?
How are problems categorized into different difficulty levels?
Does an individual problem encompass multiple domains simultaneously? How is this handled?
For evaluation, the authors directly employed GPT-4 for assessment. How to ensure the accuracy of GPT-4's evaluations, and what is the consistency with human assessments? Furthermore, it is recommended that the authors incorporate rule-based evaluation methods to enhance accessibility.
Flag For Ethics Review: No ethics review needed.
Rating: 3: reject, not good enough
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes