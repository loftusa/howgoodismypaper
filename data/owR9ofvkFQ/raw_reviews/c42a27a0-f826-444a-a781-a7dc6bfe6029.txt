Summary:
The authors identify a need for a mathematics benchmark that spans a wider breadth of topics and difficulties. They propose MathOdyssey, a benchmark that includes hand-written and curated high-school, university, and Olympic-level problems. Each problem has a unique expected answer and detailed reasoning to aid LLM assessment. They demonstrate that the benchmark is not saturated since GPT-4 o1-preview archives ~65% overall. Further, their wide coverage of topics enables the identification of problem topics for LLMs enabling researchers to focus on those areas.

Soundness: 3: good
Presentation: 2: fair
Contribution: 4: excellent
Strengths:
The proposed benchmark should have a unique answer enabling easier verification of the correct answer.
The problems are crafted specifically for the benchmark avoiding their presence in the pre-training data for LLMs.
The problem space covered in terms of topics and difficulty is wide, allowing the identification of problem areas for further research as well as "solved" areas if a topic saturates.
Weaknesses:
Some of the paper language is hyperbolic, for example, S3.1, "Design Principle." Paragraph, L178: "representing the pinnacle of human intellectual achievement" is very strong language and am uncertain the authors could substantiate such a claim unless it is an opinion. Another example is S3.1 L234: "This rigorous process": The curation process, while I can trust was done rigorously, is not presented in sufficient detail for me to make that assessment, and it would be better to instead tone it down to "This process facilitates the quality and dependability...". The paper in general would benefit from a pass that tones down the hyperbolic language to instead focus on the proposed advancements in an objective tone.
L256-264 could be replaced with "Fig. 1 shows the detailed information". and L266-268 repeat the same information that is already in the figure and could be pointed to.
The benchmark claims easy verifiability by code but uses GPT-4 as an evaluator (with the associated errors this induces even if the prompt enables the use of tools).
The ease of having a unique answer is counter-balanced by the false positives induced by correct-answer-with-wrong-reasoning.
Questions:
Is there a particular reason that an LLM is used as an evaluator instead of using SymPy or Mathematica directly when needed (for example equivalence of symbolic expressions)?
While I have not deep dove into the dataset, the example from Table 1 has implicit or missing steps in the university example to my reading. It seems to have an implicit application of L'HÃ´pital's Rule before taking the limit and then applying the chain rule. Thus, my question is how did you balance implicit vs explicit steps in the reasoning? I would expect/hope an LLM to list all steps, including those "obvious" to a human marker, and err on the verbose side; however, I accept this as a personal view/bias and am keen to hear what the dataset prerogative was.
Flag For Ethics Review: No ethics review needed.
Rating: 6: marginally above the acceptance threshold
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes