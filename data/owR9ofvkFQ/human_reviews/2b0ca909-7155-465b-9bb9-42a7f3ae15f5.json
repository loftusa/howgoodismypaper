{"summary":"The manuscript presents an original and challenging dataset for mathematical problem-solving, encompassing various subjects and difficulty levels. Then the paper conducts comprehensive examinations on both open-source and closed-source LLMs. The findings reveal that while closed-source models currently lead, open-source models are rapidly catching up, highlighting the competitive landscape of LLM capabilities in mathematical problem-solving.","soundness":"2: fair","presentation":"2: fair","contribution":"2: fair","strengths":"The paper is well-motivated, as GPT-4o poses a significant challenge to current mathematical benchmarks. The introduction and open-sourcing of high-quality, difficult mathematical problems is a meaningful contribution to the field.\n\nThe dataset features distinct levels of difficulty and sub-domain classifications, which enhance its uniqueness.","weaknesses":"The manuscript lacks coverage of important related work and further clarification on the difference and improvements compared to them.\n\nOlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems\nOmni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models\nOlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI\nHave LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models\nThere are missing baseline comparisons that are crucial for evaluating the open-source models on the proposed dataset, like Qwen2.5-MATH, DeepSeek-Coder, and so on.\n\nIt is unclear how the authors ensure that the data has not been previously encountered. If the problems are original, details regarding the creation principles and methodologies should be included in the paper. Additionally, how is the correctness of answers verified? Have the authors conducted cross-validation or sampling tests to ensure reliability? What is the accuracy rate?\n\nThe conclusions drawn seem predictable and do not provide substantial insights. Are there fine-grained analyses and interesting findings?","questions":"While the authors mention diverse answer types, there are only three categories presented. Moreover, there seems to be a discrepancy between the data in Figure 2 and that described in line 304. Given that existing mathematical models have undergone extensive pre-training and can effectively comprehend natural language queries, why does the diversity of answer types pose a greater challenge for these models? Is there a variance in accuracy across different question types? Do models exhibit inconsistencies when dealing with various types of questions?\nThe creation details of the problems:\nPlease include principles and details of problem creation in an appendix. How is answer correctness ensured? Have cross-validation or sampling methods been employed? What is the accuracy rate?\nHow are problems categorized into different difficulty levels?\nDoes an individual problem encompass multiple domains simultaneously? How is this handled?\nFor evaluation, the authors directly employed GPT-4 for assessment. How to ensure the accuracy of GPT-4's evaluations, and what is the consistency with human assessments? Furthermore, it is recommended that the authors incorporate rule-based evaluation methods to enhance accessibility.","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"3: reject, not good enough","confidence":"3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.","code_of_conduct":true}