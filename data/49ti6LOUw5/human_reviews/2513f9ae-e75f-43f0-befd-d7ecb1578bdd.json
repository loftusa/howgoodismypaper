{"summary":"This article proposes a new method called UNOLORA, which utilizes shared low-rank adaptation (LoRA) modules to achieve efficient multi-task learning for large language models, and has achieved outstanding performance on the GLUE benchmark.","soundness":"3: good","presentation":"2: fair","contribution":"2: fair","strengths":"The method proposed by the authors is simple but effective.","weaknesses":"The writing and presentation is not good, for example, the caption and figure of Figure 1 seems confusing. Also the font size in the figure is too small to understand.\n\nThe training of Shared Hypernetwork will introduce additional training cost.\n\nThe method is only evaluated on one model, without scaling up the model size/architecture.","questions":"What is the difference between the UNOLora* and UNOLoRA? I haven't found the method difference in your paper?\n\nIt required a comparation to use LoRA to multi task training.\n\nIt is not clear why cross task relation is related to the capability of using LoRA to do multi-task learning.","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"3: reject, not good enough","confidence":"3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.","code_of_conduct":true}