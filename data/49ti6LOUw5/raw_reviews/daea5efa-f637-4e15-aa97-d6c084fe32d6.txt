Summary:
This paper presents a method called UnoLoRA, a procedure for constructing low-rank Transformer adapters in a multi-task setting by training a network to apply task-specific transformations to a shared adapter. In particular, while standard LoRA parameterizes weight matrices as 
 for low-rank 
 and 
, UnoLoRA parameterizes them as 
, where 
 is a task representation that includes both a discrete identifier and example data and positional embeddings, and 
 is a hypernetwork. A similar recipe was previously explored by Karimi Mahabadi et al. (2021) under the name of "HyperFormers"; as far as I can tell, the main differences are that:

HyperFormers condition only on task IDs, while UnoLoRA conditions on example input data

HyperFormers also modulate LayerNorm parameters, and not just adapters

HyperFormers use a slightly different adapter parameterization from the modern LoRA recipe, with a nonlinearity in the middle

Soundness: 1: poor
Presentation: 1: poor
Contribution: 2: fair
Strengths:
Simple and seemingly effective way of parameterizing low-rank adapters in the multitask setting. The idea is timely---there have been a lot of improvements in LoRA and related schemes in the last couple of years, and revisiting conditional computation + adapter combinations seems like a promising direction.
Weaknesses:
Comparatively minor tweak of an existing idea. This wouldn't be an issue on its own, except for the fact that the various changes are not evaluated in a way that enables direct comparison to HyperFormers, as described below.

Inconsistencies and missing details in the description of the method. Fig 1 makes reference to a "Task-specific A" parameter that is not mentioned anywhere in the formal description of the method---is it used, and if so, where? Additionally, the experiments make reference to a method called UnoLoRA
, which achieves slightly better performance than the base method but does not appear to be described anywhere.

Major issues in evaluation. The paper's main results are summarized in Fig 6(a), which show that UnoLoRA and HyperFormers both pareto-dominate training separate adapters for each task---UnoLoRA involves fewer parameters at the same level of performance, while HyperFormers give increased accuracy but are slightly less parameter-efficient than UnoLoRA. I have two concerns here.

First, the individual differences between UnoLoRA and HyperFormers are never individually evaluated, making it impossible to figure which (if any) are responsible for the performance differences.

Second, and more fundamentally---the whole point of adapter-based methods is that they provide a tunable parameter (the adapter rank) that trades off between accuracy and parameter count. So what we really need to see is the entire accuracy / efficiency curve for both model classes, rather than an arbitrary point on each. In fact, if I understand correctly, even the size of the adapter is totally incomparable between the two models being compared: this paper trains UnoLoRA with a rank of 8, while the results copied from the HyperFormers paper appear to use a rank of 24.

Without a minimal comparison (or a complete frontier from each model), it is possible that all observed differences between methods result from incomparable hyperparameter choices.

Major formatting issues: nearly every citation in the paper is incorrectly formatted (using \citet instead of \citep). It seems likely that this paper didn't receive even a single round of proofreading, and should not have been submitted to ICLR in its current form.

Questions:
How does performance change as rank is varied?
How do individual components of the method affect performance?
What is UnoLoRA
?
Is there a task-specific 
 matrix or not?
Flag For Ethics Review: No ethics review needed.
Rating: 3: reject, not good enough
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes