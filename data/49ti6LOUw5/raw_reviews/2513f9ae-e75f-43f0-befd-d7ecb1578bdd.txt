Summary:
This article proposes a new method called UNOLORA, which utilizes shared low-rank adaptation (LoRA) modules to achieve efficient multi-task learning for large language models, and has achieved outstanding performance on the GLUE benchmark.

Soundness: 3: good
Presentation: 2: fair
Contribution: 2: fair
Strengths:
The method proposed by the authors is simple but effective.
Weaknesses:
The writing and presentation is not good, for example, the caption and figure of Figure 1 seems confusing. Also the font size in the figure is too small to understand.

The training of Shared Hypernetwork will introduce additional training cost.

The method is only evaluated on one model, without scaling up the model size/architecture.

Questions:
What is the difference between the UNOLora* and UNOLoRA? I haven't found the method difference in your paper?

It required a comparation to use LoRA to multi task training.

It is not clear why cross task relation is related to the capability of using LoRA to do multi-task learning.

Flag For Ethics Review: No ethics review needed.
Rating: 3: reject, not good enough
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes