22 October 2023, 14:30 (modified: 22 October 2023, 14:30)
Summary: This paper presents a theoretical analysis of dropout in Graph Convolutional Networks (GCNs), exploring how dropout uniquely interacts with graph structure. The authors demonstrate that dropout in GCNs creates dimension-specific stochastic subgraphs, leading to structural regularization not found in standard neural networks. The analysis reveals that dropout effects are degree-dependent, with nodes of higher degree experiencing less regularization variation. The paper provides insights into dropout's role in mitigating oversmoothing, derives generalization bounds specific to GCNs with dropout, and examines how dropout and batch normalization interact synergistically. The theoretical findings are validated through extensive experiments on both node-level and graph-level classification tasks across 14 datasets.

Soundness: 3
Presentation: 3
Contribution: 3
Strengths: The paper presents a novel theoretical framework for understanding dropout in GCNs, addressing an important gap in the literature. The theoretical contributions are substantive, particularly the insights into dimension-specific stochastic subgraphs and degree-dependent regularization. The empirical validation is thorough, encompassing both node-level and graph-level tasks across multiple datasets. The analysis of the synergistic interaction between dropout and batch normalization provides practical insights for GCN implementation. The authors demonstrate that their theoretical insights lead to practical performance improvements, with their dropout+BN approach outperforming several state-of-the-art methods on benchmark datasets.

Weaknesses: Some of the theoretical proofs are quite complex and could benefit from more intuitive explanations to make them accessible to a broader audience. The paper introduces many concepts and theorems in succession, which sometimes makes it difficult to follow the core narrative. While the experiments are comprehensive, they don't clearly isolate the specific effects predicted by the theoretical analysis (e.g., explicitly testing degree-dependent regularization across different node degree categories). The comparative analysis with other regularization techniques for GNNs is somewhat limited, missing opportunities to benchmark against methods like DropEdge or GraphSAGE's node sampling.

Questions: How would the proposed insights on dropout in GCNs extend to dynamic graphs where the structure changes over time? Could the authors elaborate on how the degree-dependent regularization might be exploited to design better graph sampling strategies for large-scale GNNs? The paper mentions that high-degree nodes experience less variation in their effective degree - how might this property be leveraged to design more robust GCNs for graphs with highly skewed degree distributions? How sensitive are the performance improvements to the specific choice of dropout rate, and does this sensitivity correlate with graph structural properties as the theory would suggest?

Flag For Ethics Review: No
Rating: 8
Confidence: 4
Code Of Conduct: Yes