{"summary":"This paper introduces a multi-scale learning approach for medical time series classification. The proposed method comprises multiple independent models, each with a distinct patch length, allowing it to capture information across various temporal scales. The patching method follows the PatchTST framework, which employs single-channel patching. To reduce computational costs, the authors implement stochastic sparse sampling, randomly selecting models during training. The final representation is an aggregation of outputs from all models, combining multi-scale information. The model is evaluated on intracranial EEG (iEEG) data for seizure onset zone classification, using a dataset collected from four independent medical centers.","soundness":"2: fair","presentation":"2: fair","contribution":"2: fair","strengths":"The use of sparse sampling for computational savings in multi-scale learning is an interesting idea. Additionally, the out-of-distribution classification on unseen subjects from different medical centers demonstrates strong potential for generalizability in real-world applications.","weaknesses":"The motivation to save computational resources is well-intentioned, though I am concerned about its practicality in actual training. For a given set of window sizes with corresponding independent models, even if only subsets of window sizes are selected during training, the space complexity of the models remains unchanged. This approach primarily improves training speed without reducing memory requirements. Additionally, using an independent model for each patch length may not be optimal for memory efficiency. A shared backbone across different patch lengths could be a more effective choice for memory savings. Overall, the method resembles an enhanced version of MTST [1], employing a random subset of models with varying patch lengths during training.\n\nMoreover, while the paperâ€™s title refers to medical time series, only a single seizure dataset is used for evaluation. Expanding the evaluation to include additional datasets would strengthen the claim of generalizability. The ablation study could also benefit from a deeper investigation into multi-scale learning with various patch lengths. For instance, exploring which combinations of patch lengths yield the best performance would be informative. Additionally, the impact of stochastic sparse sampling should be assessed in detail. For a given list of patch lengths, how do memory usage and running time between training with and without stochastic sparse sampling? Lastly, a recent work, Medformer[2], should be compared in baseline methods, as it is also designed for medical time series classification using multi-scale patching. A discussion on the differences between this method and Medformer would also be valuable for highlighting the unique aspects of the proposed approach.\n\n[1] Multi-resolution Time-Series Transformer for Long-term Forecasting [2] Medformer: A Multi-Granularity Patching Transformer for Medical Time-Series Classification","questions":"See Weaknesses","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"5: marginally below the acceptance threshold","confidence":"5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.","code_of_conduct":true}