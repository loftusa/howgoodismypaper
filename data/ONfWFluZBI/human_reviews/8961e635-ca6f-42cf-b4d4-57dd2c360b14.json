{"summary":"The paper explores the use of self-supervised contrastive learning for dynamic system identification. It connects self-supervised learning (SSL) with identifiable representation learning, showing that SSL can identify system dynamics in latent space. The authors propose a model to uncover linear, switching linear, and non-linear dynamics under a non-linear observation model, providing theoretical guarantees and empirical validation.","soundness":"3: good","presentation":"3: good","contribution":"3: good","strengths":"The connection between contrastive learning and dynamic system identification is novel and could lead to simple encoder-only implementations favored in practice.\nThe synthetic experiments, especially the ablation studies, are extensive and investigate many aspects of the theoretical results.","weaknesses":"The contributions (comparison with previous work), theoretical techniques, and novelty are very lightly discussed. I would appreciate a detailed discussion comparing this work's conditions with those in recent literature on temporal causal representation learning (e.g., [1] and its follow-up work). This would aid the contextualization of this work and make the contribution more transparent.\n\nThis work strikes me as theoretically oriented. There is a lack of discussion on the theoretical conditions, limitations, and implications. Just to name a few:\n\n(a) the theorem requires the length for each time series to be infinite, which appears to be quite a strong assumption and not necessitated in recent work (e.g., [1], admittedly I am not an expert and would appreciate correction if I have misunderstood the statements). Why is this necessary, and would it be possible to show results for finite-length series?\n\n(b) How does the control distribution influence the identification results? what conditions should it meet (does it have to be a time-independent Gaussian distribution)?\n\n(c) Line 175: what is the significance of \n -- is it previously defined? Could you comment on lines 175-177?\n\nSome minor issues:\n(a) Line 145: what is (...)?\n\n(b) Line 165: shouldn't \n be a matrix?\n\n[1] Temporally Disentangled Representation Learning. Yao et al. NeurIPS 2022.","questions":"See the weaknesses section.","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"6: marginally above the acceptance threshold","confidence":"3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.","code_of_conduct":true}