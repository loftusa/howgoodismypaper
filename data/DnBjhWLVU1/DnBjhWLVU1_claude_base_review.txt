23 October 2023, 15:30 (modified: 23 October 2023, 15:30)
Summary: This paper introduces a novel weight regularization method called Soft Weight Rescaling (SWR) designed to address the issue of plasticity loss in neural networks. As neural networks are trained, unbounded weight growth causes them to lose the ability to learn new information. SWR works by scaling down weights proportionally to their growth from initialization, preserving the network's functionality through maintaining proportionality while preventing unbounded growth. The authors theoretically prove that SWR bounds weight magnitudes and balances weights between layers. Through experiments on multiple datasets and model architectures in warm-start learning, continual learning, and single-task learning, SWR demonstrates superior performance compared to existing weight regularization and re-initialization methods.

Soundness: 3
Presentation: 3
Contribution: 3
Strengths: The paper makes a clear contribution by introducing a novel weight regularization technique that addresses an important problem in deep learning: plasticity loss due to unbounded weight growth. The theoretical analysis is solid, proving that SWR bounds weight magnitude and improves layer balancedness. The experimental design is comprehensive, testing the method across different architectures, datasets, and learning scenarios. The results consistently demonstrate SWR's effectiveness, particularly for larger models like VGG-16 where other regularization techniques provide minimal benefits. The paper is well-structured and the motivations are clearly articulated.

Weaknesses: The paper lacks comparisons with more recent or sophisticated methods for maintaining plasticity, such as those that employ Bayesian approaches or more advanced regularization techniques. For the warm-start and continual learning experiments, there's limited analysis of how SWR affects feature representations and what precisely causes the improved performance. The theoretical analysis, while sound, doesn't provide tight bounds on the Lipschitz constant, which could be valuable for understanding network stability. Some parameter selection details are relegated to the appendix, making it harder to understand how sensitive the method is to hyperparameter choices. The empirical validation sometimes shows L2 regularization performing better in certain scenarios, but there's insufficient discussion about when and why SWR might underperform.

Questions: 1. How sensitive is SWR to the choice of Î» parameters, and is there a principled way to select them beyond grid search?
2. The method rescales weights but preserves proportionality between layers. How does this approach compare to methods that explicitly optimize for layer-wise balance?
3. In continual learning scenarios, how does SWR's performance compare to methods specifically designed for continual learning (e.g., EWC, PackNet, or other parameter isolation methods)?
4. Is there a theoretical relationship between SWR and other regularization methods like L2 or spectral normalization?
5. The experiments use standard image classification tasks. How might SWR perform on other tasks like NLP or reinforcement learning where weight dynamics might differ?

Flag For Ethics Review: No
Rating: 8
Confidence: 4
Code Of Conduct: Yes