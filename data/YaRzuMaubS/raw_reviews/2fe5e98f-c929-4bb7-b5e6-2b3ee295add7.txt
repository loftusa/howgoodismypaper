Summary:
This paper presents a formalisation of deception in terms of POMDPs (partially observable Markov Decision Processes). Two agents, a speaker and a listener have their own PODMDPs. The listener has a model of the speaker's policies, and the speaker has a probability distribution over this model.

The speaker communicates information to the listener. The speaker may be honest or deceptive (including deception by omission), and the listener takes actions based on their model of the speaker's honesty or deception.

Then deception is defined in terms of the listener's regret. This regret is measured, in the paper, via two reward functions: the listener's true reward function (which measures how much they have been harmed by the speaker's possible deception) or a reward function based on the accuracy of the listener's beliefs (which measures how much they have been mislead by the speaker's possible deception).

Some human feedback experiments on simulated data show that human judgments of deception are somewhat correlated with these two regret measures, with the correlation being stronger with the accuracy-of-belief based regret.

Soundness: 3: good
Presentation: 3: good
Contribution: 2: fair
Strengths:
The formalism is fine and intuitively plausible (though somewhat idealised). The experiments are well executed and well presented.

Weaknesses:
The formalism is interesting, but is only a mild variation of multi-agent POMDP (and MDP) formalisms from other papers (see, eg CIRL papers such as "Cooperative Inverse Reinforcement Learning"; searching for competitive MDP and POMDP papers will give many other examples).

It is not that the formalism is exactly the same as previous formalisms, but that it is very similar to many of them. Nevertheless, the formalism is fine (as mentioned above), and, if it lead to powerful examples and demonstrations, would be an excellent introduction to a great paper. But without those powerful examples or demonstrations, it is not enough to make the paper worthwhile in itself. The experiments show that there is a certain overlap between human judgements and these measures (especially the second regret measure with accuracy of beliefs as the reward), but all the correlations, bar one, are below 0.5, and "humans weakly agree with this regret measure in three examples" is not enough meat on the bones for this paper.

What the paper needs is a great use case for this formalism, powerful experiments that show its use. An intuitively plausible definition is not enough.

Questions:
Find a more powerful use case for the formalism. Fine-tuning an LLM with it, running it over a very large database of deceptions, showing its validity when compared with other literatures (eg the psychological literature), running it on online games where there is a way for players to demonstrate trust or lack thereof,... These are some suggestions, and I'm sure that the authors can come up with better ones. Find results so powerful that readers don't think "this is a plausible and interesting formalism", but "wow, this formalism really illuminates things/leads to impressive improvements".

Flag For Ethics Review: No ethics review needed.
Rating: 5: marginally below the acceptance threshold
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Code Of Conduct: Yes