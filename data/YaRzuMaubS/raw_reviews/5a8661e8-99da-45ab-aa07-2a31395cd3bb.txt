Summary:
The paper provides a formal definition of deception in a setting where a Speaker and a Listener interact with each other over multiple rounds. The goal of the proposed definition is to capture the Speaker's degree of deception by measuring to what extent its action(s) affect the Listener's beliefs and individual utility. The framework that is used to formalize the Speaker-Listener interactions is a POMDP variation, termed as Communication POMDP, where the Listener is modeled as part of the environment and the Speaker plays the role of the acting agent. Under this decision making framework, the Speaker's degree of deception is formalized through a function that measures the Listener's regret w.r.t. the accuracy of its beliefs and its personal utility, that results from the Speaker's actions. Finally, the paper includes extensive experimentation over a diverse set of scenarios that aims to showcase how well the proposed approach aligns with human intuition and how it compares to LLM baselines.

Soundness: 4: excellent
Presentation: 2: fair
Contribution: 2: fair
Strengths:
The paper is well-structured and the work well-motivated. Related work is sufficiently covered and the gap that the proposed definition aims to address is clearly highlighted.

The problem of detecting or preventing deception in AI systems is a very important one, and has become especially relevant these days with all the recent advancements in the field. The main idea promoted by this paper, i.e., that a formal definition for measuring deception is needed, is indeed crucial for making progress in this problem. To the best of my knowledge, this is the first work that attempts to provide such a definition that does not merely measure the truthfulness of agent's statements but instead looks deeper into the intricacies of the problem.

The Speaker-Listener setting on which this paper focuses does not of course capture all real-world decision making scenarios where detecting deception would be useful, but I find it general enough. The regret function proposed for measuring deception is simple to understand and quite reasonable, although far from complete as also mentioned in the Limitations section of the paper (see also Weaknesses below).

The biggest strength of the paper in my opinion is the experimental evaluation of the approach. I find it to be rigorous and very well-thought. Even though the proposed method does no do that well in all metrics, it significantly outperforms the LLM baselines.

Weaknesses:
Presentation: My main issue with this paper is Section 2.2 and the proposed POMDP framework. First, I found the presentation of this section to be quite poor: (a) Many things are not adequately explained, e.g., it is not explicitly mentioned that the world state does not change over time; (b) There are non-standard parts of this framework that are not revisited later on in the paper. This can hinder the understanding of the reader w.r.t. the role of these notions in the framework and why they are needed, e.g., caligraphic omicron in lines 162-164 -> no matter how many times I read this part I could not understand what is used for or what it means; (c) There are notions that are mentioned first and introduced later, e.g., reward r_L which shows up in line 145 and later in line 204 is properly explained in Section 2.4 for the first time. In general, I believe that this paper suffers from serious notational issues which can bring confusion to the reader, e.g., I understand what is the difference between states s_S and s_L but it was never made clear what plain s (Equations 1, 2, 3) stands for, to my understanding is the same as s_L but I am also not sure.

Framework complexity: Regarding the Communication POMDP framework, I find it overly complicated and poorly motivated, why is this the correct framework to use? Given that the main purpose of this paper is to be the starting point for research on formalizing deception, I believe that a more comprehensive framework would be quite more helpful. Furthermore, the regret notion in Section 2.3 seems quite simple, which does not justify why all this complexity in the proposed framework. Even though, I have not worked out the details, it seems that turn-based Dec-POMDPs [1, 2, 3] with two agents, Listener and Speaker, could be a potentially suitable framework for expressing your regret notion.

Definition: I appreciate the honesty in the limitations section, and I think that since you present your solution as a starting point this part is fine. I believe however that your definition has one additional important weakness that is not mentioned in the Limitations section. In case, Speaker tries to deceive Listener, but the latter does not trust the former and hence it is not influenced by its actions, your approach would classify Speaker as not deceitful, even though it is.

[1] Sidford, Aaron, et al. "Solving discounted stochastic two-player games with near-optimal time and sample complexity." International Conference on Artificial Intelligence and Statistics. PMLR, 2020.

[2] Jia, Zeyu, Lin F. Yang, and Mengdi Wang. "Feature-based q-learning for two-player stochastic games." arXiv preprint arXiv:1906.00423 (2019).

[3] Frans A. Oliehoek and Christopher Amato. 2016. A concise introduction to decentralized POMDPs. Springer

Questions:
Q1: In lines 142 143 is it \pi or \hat{\pi}?

Q2: In section 2.4 I do not understand how the reward in Eq. 3, i.e., the belief state, is equivalent to measuring the accuracy of the belief state. Could you maybe further explain this part?

Q3: How would you need to adapt your definition in the presence of multiple Speakers, in order to measure the individual degrees of deception?

Q4: Why is Communication POMDP the suitable framework for this problem? What is the rationale behind modeling Listener as part of the environment and not as a second acting agent with its own policy and objective?

Suggestion: I think it would be useful for the exposition of the experimental results if you bring Figure 3 to the main paper. If I am not mistaken the whole point of ICLR allowing an extra page this year was to account for such things.

Flag For Ethics Review: No ethics review needed.
Rating: 3: reject, not good enough
Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.
Code Of Conduct: Yes