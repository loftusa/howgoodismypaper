{"summary":"This paper presents a new quantitative definition of deception in agent-to-agent dialogue interactions modelled as partially observable Markov decision processes (POMDPs). One of the agents is the speaker (S), and the other agent is the listener (L). The listener L is modelled as a POMDP over a certain set of world states. The set of observations of the POMDP representing L is the same as the set of actions available to S. The speaker S is modelled as a POMDP over a state space that contains the world states plus belief states over L. Essentially, S has access to the ground truth of the world and the actions provided by L, and has a belief model over the beliefs and policies of L, while L has no access to the ground truth, and has a belief model over the ground truth guided by the actions of S.\n\nEach agent (S and L) has its own reward function.\n\nDegree of deception is defined as the difference between the expected reward of L if it listens to (and therefore updates its beliefs over) S, and the expected reward of L if it does not listen to (and therefore does not update its beliefs over) S. This way, a positive value indicates an altruistic speaker (S makes the reward of L larger if L listens to S), a negative value indicates a deceptive speaker (S makes the reward of L smaller than it would be if L did not listen to S), or neutral if they are the same. (*)\n\nThe paper states that different concepts of deceptiveness can be captured by plugging different reward functions in Eq. (1). In particular, deceptiveness can be defined as (i) S producing worse outcomes (reward for deception = reward for the task), (ii) S producing beliefs in L further from the truth (reward for deception = belief is close to reality), or (iii) a combination of both (i) and (ii).\n\nArmed with this definition, the paper presents an experimental study, where they generate scenarios with different pairs of agents, ask human subjects to rate the degree of deceptiveness and compare the results with the degree of deceptiveness given by Eq. (1), as well as the degree of deceptiveness when substituting human subjects by state of the art large language models (LLMs). The paper claims that their results support the hypothesis that their definitions of deceptiveness align with human intuition, especially when the reward combines outcome and beliefs, and that the alignment with human intuition is much better than that given by state of the art LLMs.\n\n(*) This is inverted to what is stated in the text (lines 198-209). I believe there is either a negative sign missing in Eq. (1) or a mismatch in describing the equation in lines 208-209. In any case, it is at the level of a typo, it does not significantly affect the contribution or quality of the paper.","soundness":"3: good","presentation":"3: good","contribution":"2: fair","strengths":"S1. The topic of deception in autonomous systems is relevant and timely. The use of autonomous systems in day-to-day decision-making is increasing (especially with the current development of LLMs), and it is fundamental to have models of different intentional harms to increase trust in these systems by the public and accountability for potential harms caused by them. Deceptive language, especially in interactive systems, is of particular relevance.\n\nS2. The paper is written nicely, with a strong motivation, clear structure and understandable examples to guide the reader.\n\nS3. As an experimental evaluation, the paper includes a study on human subjects and addresses state of the art LLMs. The scenarios presented are a good balance of simple and realistic.\n\nS4. The paper has a fair discussion on the limitations of their approach, mentioning how there is work to be done to deploy the proposed concept in more complex and realistic scenarios.\n\nS5. The paper engages with current literature on different definitions of deception and use of LLMs in relation to deception.","weaknesses":"MAIN WEAKNESSES\n\nW1. The formalism has no clear novelty. Defining deceptiveness as the level of regret just passes the ball of defining deceptiveness to the reward function. This is not in itself a bad decision, but it does mean that the interest does not lie so much in the definition as proposed in Eq. (1), but rather lies in the choice of reward function. The choice of reward function feels underexplored to me as part of the experimental report.\n\nW2. I think the experimental evaluation goes in the right direction, and most of the data obtained will be useful, but I find it weak as it is now. I will structure my criticism into points that I believe are misleading and points that I believe are incomplete. I also number them, to facilitate later discussion.\n\nW2.1. Misleading.\n\nW2.1.1. In Table 1, the numbers presented indicate the correlation between human perceived deceptiveness and the values given by regret and by LLMs. In the table it states that these results were statistically significant, with a p-value < 0.001. I do not see in the text what statistical test this p-value refers to, I can only assume by context that the null hypothesis was \"there is no correlation between human ratings and machine ratings\". If this is the case, the result is hardly surprising (although it is useful as a means of a sanity check), and I find it misleading to accompany it to the concrete values given in this table.\n\nW2.1.2. The nutritionist scenario is a bad choice, and I do not agree with the reason given to its lower correlation in lines 361-363. In the nutritionist scenario, human subjects are presented with facts that are controversial in the current public opinion (whether protein, restriction of carbohydrates or herbal teas boost energy). The human subject is going to come with its own beliefs to the task, and they would certainly influence their perception of deceptiveness. I think the nutritionist example is a bad choice and without having prior information on the beliefs of the human subjects with respect to protein, carbohydrates and teas, no reliable conclusions can be extracted from the data of that experiment.\n\nW2.1.3. Lines 355-356 state that \"We largely find that a combined regret formulation better captures human intuitive notions of deception across all three scenarios, confirming our hypothesis from Section 2.3 that both belief and task reward contribute to improving the correlation with human judgment\". While it is true that the \"Combined\" column is larger than the \"Belief\" column, it is not by much. It would be helpful to accompany this statement with a statistical test and its corresponding p-value.\n\nW2.2. Incomplete.\n\nW2.2.1. Lines 368-373 include information about multi-step conversations. It would be useful to have a table similar to Table 1 summarizing the information, maybe in an appendix if it does not fit in the main text. Also, a conclusion is given that the correlation between humans and regret is higher for multi-step conversations. It would be helpful to accompany this statement with its corresponding statistical test and p-value. It would also be interesting to know how much (if any) the LLMs improve in multi-step conversation.\n\nW2.2.2. One of the conclusions derived from the experimental evaluation is that the presented regret-based formalism aligns better with human intuition than the estimation given by LLMs. Again, it would be interesting to know the statistical significance of this statement, but more importantly, it would be interesting to understand why. Given the 1-5 scale, it is possible that the LLM produces a less extreme (but still on the correct side) value than the human (for example, the LLM would choose 2 instead of 1, or 4 instead of 5). This would produce a smaller correlation, while indicating that the LLMs are still aligned with human intuition. Another possibility is that the lower correlation comes from the LLMs contradicting human intuition (i.e. the LLM choosing a value >3, when the human chooses a value <3, and vice versa). Of course, in reality, it is probably a combination of both phenomena. It would be however very informative to include some information about this, maybe as part of a qualitative analysis (currently Sec. F in the appendix).\n\nW2.2.3. One of the questions in the study is which reward function produces deceptiveness degrees that best align with human intuition, and the winner is the \"combined one\". However, as far as I can tell, it is not stated in the paper what is the weight used in combining these values. As an extra step, it would also be interesting to see how the correlation varies for different weights, and whether an \"optimal\" weight arises from the experiments.\n\nI hope the authors do not get discouraged by this review. I really like the study presented in the paper and think it has much value. With some additional depth and clarity in the experimental evaluation, this paper could be a strong contribution for a top-tier venue like ICLR in the future.\n\nOTHER (MINOR) REMARKS\n\nThese are smaller remarks, mostly editing issues. I hope the feedback serves to polish the paper.\n\nR1. The bibliography needs to be polished. Here is a list of issues I found.\n\nR1.1. There are repeated items, for example [He et al. 2018], [Lewis et al. 2017], [Wang et al. 2020].\nR1.2. There are many items that lack a journal, conference, arxiv id or similar. I know in the era of the internet one can find papers just from the title, but let's keep good practices. For example [Abdulhai et al. 2023], [Amodei et al. 2016], [Bai et al. 2022], [Sung et al. 2023], [Pan et al. 2023], [Park et al. 2023], [Touvron et al. 2023], [Ward et al. 2023], [Wei et al. 2023].\nR1.3. There are typos: <i>diplomacy</i> in [Bakhtin et al. 2022], missing capitalization in [Greene 2007], [Wang et al. 2021] .\nR1.4. This may be a quirk of mine, but I find it misleading to cite papers that have been presented at major AI/ML venues giving only their arxiv id, while other papers are cited in the proceedings of some conference or journal. For example [Aakanksha et al. 2022] appeared in JAIR 2024, [He et al. 2018] in EMNLP 20218, [Lin et al. 2021] in ACL 2022, or [Pan et al. 2023] in ICML 2023. I would appreciate at least consistency.\nR1.5. I don't think [Brown et al. 2020] is an appropriate citation for GPT3.5-Turbo. See the discussion here for example: https://community.openai.com/t/how-to-cite-text-davinci-003-in-academic-paper/369821.\nR2. Apart from the missing details mentioned in W2, there are some missing definitions. When defining POMDP (line 104), the set of beliefs (B) is mentioned, without stating what it is. I assume that a belief is a probability distribution over states (as usual), but this should be stated for the sake of completeness. On a similar note, it is not clear to me what b_L(s) means in Eq.(3). To my understanding, b_L is a probability distribution of states, and b_L(s) is the probability of state s. If so, I would find it more suitable for the reward function to be a distance between the probability distribution b_L and the probability distribution where \"s\" has probability 1, and the rest of the states have probability zero. This is a small change, but as a reader, I spent some time having to think over the definition because it was missing. This distracting confusion could be easily avoided by providing the complete definitions.\n\nR3. The authors could consider engaging with the recent literature of explainable RL in terms of deception and intentional behaviour, and how this can be used to analyse harmful behaviours (deception being here a harmful behaviour). See for example:\n\nLiu, Z. et al. Deceptive Reinforcement Learning for Privacy Preserving Planning. AAMAS 2021.\nLewis, A. et al. Deceptive Reinforcement Learning in Model-Free Domains. ICAPS 2023.\nCordoba, F.C. et al. Analyzing Intentional Behavior in Autonomous Agents under Uncertainty. IJCAI 2023.\nBeckers, S. et al. Quantifying Harm. IJCAI 2023.\nR4. These are just two suggestions:\n\nI would make section 2.4 significantly shorter to allow for more details on the experimental evaluation. The definition is easy enough to grasp, it may not need so much redundant explanations.\nIn lines 256-257 I would put a different example.\nR5. The paper needs an editorial pass. Here is a list of typos and the like.\n\nobtains --> obtained (line 079)\nFigures 3, 4, and 5 are mentioned in the main text without stating that they are part of the appendix.\nReference to Fig. 4 in line 315-316 should be to Fig. 6.\nI do not understand the point of Fig. 4 at all. Does it add any insight that is not already provided by Fig. 6?\nleft --> right (line 931).\nThe caption of Fig.4 is misleading, and seems to be referring to Fig. 5 instead.\nIn Figure 7, the image representation of practising photography and being part of community events are swapped. If this is not only a typo in the paper, but this is also how the examples were presented to the human subjects and LLMs, this fact should be stated somewhere, or the experiment repeated.\nTable 2 is not mentioned in the text.\nThe font in Figures 3 and 4 is significantly smaller than the main text. Consider making it larger to improve readability. Especially in the appendix, where the page limit does not apply.","questions":"Q1. How did you choose the reward functions in the combined metric? Was any hyperparameter tuning done for the relative weight of the outcome and belief terms of the combined reward?\n\nQ2. Can you provide some insight in why the correlation with LLMs is so low?\n\nQ3. Was there any demographic selection criteria for the participants in the study, like concrete age ranges, gender, race, etc?\n\nQ4. Any comment about the missing information in W2 is welcomed.","ethics_flag":"No ethics review needed.","ethics_concerns":null,"rating":"3: reject, not good enough","confidence":"4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.","code_of_conduct":true}