{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndif reviewer scores\n",
    "reviewer_1 = \"\"\"\n",
    "Nov 2024, 12:56 (modified: 02 Dec 2024, 10:19)\n",
    "Summary:\n",
    "The paper introduces two systems NNsight and NDIF that collectively aim to reduce the developer and hardware costs of analyzing and modifying the inference behavior of open-source models. NNsight is an instrumentation framework for PyTorch models, while NDIF is an inference service that enables deferred execution of NNsight instrumentations on a remote shared model deployment. The paper provides evaluation comparing to inference baselines of shared (Petal) and non-shared HPC deployments.\n",
    "\n",
    "Soundness: 3: good\n",
    "Presentation: 3: good\n",
    "Contribution: 3: good\n",
    "Strengths:\n",
    "This paper is focusing on an important problem since tools that enable introspection of model internals are extremely valuable for ML research and applications.\n",
    "Opting for Pytorch-native tools is a great design choice as that significantly reduces development and integration burden for practitioners.\n",
    "Enabling resource sharing via NDIF service is very useful to democratizing access to SOTA models.\n",
    "Weaknesses:\n",
    "The paper seems to overclaim, particularly in the title and abstract, that the work applies to foundation models in general, when in reality it only applies to open-source models, since model internals knowledge is required to create interventions. The authors should more carefully scope the claims.\n",
    "The evaluation does not study co-tenancy scenarios where the NDIF is servicing multiple NNsight requests. This is a significant oversight considering that the resource sharing benefits of NDIF is a major contribution of the work. The authors should include results showing not just the performance implications of servicing the multiple NNsight requests but also validating the correctness of the co-tenancy features.\n",
    "Questions:\n",
    "Do users submit a pair of NNsight request and an input prompt? Or how is the input prompt for exercising an intervention generated?\n",
    "When multiple NNsight requests are submitted, are all the interventions applied to a single model instance for inference, or is a separate model instance created for each request?\n",
    "If a single model instance is instrumented with multiple NNsight requests, how does NDIF ensure that a request that modifies model parameters does not affect other requests?\n",
    "How is KV-cache managed for multiple NNsight requests?\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 6: marginally above the acceptance threshold\n",
    "Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "reviewer_2 = \"\"\"\n",
    "Nov 2024, 06:59 (modified: 29 Nov 2024, 00:57)\n",
    "Summary:\n",
    "This paper introduces NNsight and NDIF, two open systems that provide efficient, transparent access to the internals of large neural networks for research purposes. NNsight extends PyTorch to offer deferred remote execution of intervention graphs, while NDIF serves as a scalable inference service that executes these intervention requests, enabling resource sharing among users. The work addresses challenges like limited access to state-of-the-art models and the significant resource demands of large-scale AI research by sharing resources.\n",
    "\n",
    "Soundness: 2: fair\n",
    "Presentation: 3: good\n",
    "Contribution: 3: good\n",
    "Strengths:\n",
    "The intervention graph extends the model computational graph and decouples experimental design from model runtime, reducing engineering complexity.\n",
    "NDIF effectively shares GPU resources among researchers (co-tenancy), reducing cost and enabling large-scale experiments\n",
    "This is a novel idea with great potential benefit to the research community.\n",
    "Weaknesses:\n",
    "The evaluation section is limited to the end-to-end performance with little detail on the system optimizations. Specifically, more information is needed to assess the performance and scalability of this system using an increasing number of users.\n",
    "While NDIF addresses large-scale experiments on open models, it does not cover closed, proprietary models hosted proprietarily.\n",
    "Lack of discussion with relevant co-serving systems like S-LoRA (Sheng et al, MLSys 2024) and dLoRA (Wu et al, OSDI 2024).\n",
    "Questions:\n",
    "Could you please share more evaluation results on the performance and scalability of the proposed systems?\n",
    "\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 6: marginally above the acceptance threshold\n",
    "Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "reviewer_3 = \"\"\"\n",
    "03 Nov 2024, 09:04 (modified: 12 Nov 2024, 11:31)\n",
    "Summary:\n",
    "This work presents a new PyTorch-compatible library for remotely accessing the internal structures of deep-neural-network-based models such as large language models (LLMs) and altering the way they operate. Using this library, it is possible to produce intervention code which is hooked into the original model to read or replace original model parameters and activations. This intervention code is translated into a directed acyclic intervention graph that augments the computational graph of the original model. Others have proposed similar, but less flexible mechanisms. For instance, pyvene (Wu et al., 2024) supports dictionary-based intervention definitions instead of the code-based interventions proposed by this work, which can also be transformed into graph-based representations and further optimised, e.g., by TorchScript. Petals (Borzunov et al., 2023), on the other hand, does not support remote execution of the intervention code, requiring it to be executed locally, which limits virtualisation opportunities and leads to additional communication overheads.\n",
    "\n",
    "Soundness: 3: good\n",
    "Presentation: 3: good\n",
    "Contribution: 2: fair\n",
    "Strengths:\n",
    "The proposed approach involves some new mechanisms that complement the related work.\n",
    "By enabling remote execution of the intervention code the authors have demonstrated a superior performance compared to Petals.\n",
    "Weaknesses:\n",
    "An interesting real-life application of the infrastructure built is missing. For instance, the authors could consider demonstrating their system on a large-scale interpretability study or model editing task that would be infeasible without their infrastructure.\n",
    "\n",
    "Questions:\n",
    "It looks like the main technical contribution of this work is the enablement of efficient/virtualised remote interventions on LLMs. Could the authors elaborate on why they believe this contribution is relevant and impactful for the ICLR community?\n",
    "\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 6: marginally above the acceptance threshold\n",
    "Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "reviewer_4 = \"\"\"\n",
    "31 Oct 2024, 11:07 (modified: 28 Nov 2024, 00:19)\n",
    "Summary:\n",
    "The paper introduces NNsight and NDIF, a framework and a cloud inference service designed to facilitate research on large-scale AI models, and enable study of the representations and computations learned by large neural networks. This work proposes a method for organizing experiments on very large models, reducing engineering burden, enhancing reproducibility, and enabling low-cost communication with remote models. The NNsight is an open-source implementation of the intervention graph architecture that extends PyTorch to support transparent model interventions without requiring local storage or management of model parameters. And the NDIF is an open-source cloud inference service that supports the NNsight by providing behind-the-scenes user sharing of model instances to reduce the costs of large-scale AI research.\n",
    "\n",
    "Soundness: 3: good\n",
    "Presentation: 3: good\n",
    "Contribution: 4: excellent\n",
    "Strengths:\n",
    "The intervention graph architecture and the NNsight system provide a practical and efficient way to decouple experimental and engineering code, making it easier to conduct complex experiments on studying the intermediate representations and gradients of large models.\n",
    "The paper includes a thorough performance evaluation comparing NNsight and NDIF with HPC and Petals, demonstrating the efficiency and effectiveness of the proposed solutions.\n",
    "The performance comparisons between NNsight and other frameworks like baukit, pyvene, TransformerLens show the similar efficiency of NNsight.\n",
    "The remote execution of NNIF provides the easy usage for users.\n",
    "Weaknesses:\n",
    "While the developed tools of NNsight and NDIF are very useful in scientific study of AI models, there is no founded insights of this work. I'm not sure whether this contribution is enough for the acceptance.\n",
    "Exploiting the DAG representation is not novel. As indicated by the authors, [1] has proposed the DAG concepts and use it to accelerate computation. Besides, the Torch Profiler [2] and torch.jit.trace [3] can help to dissect the model. I do not see any obvious technical challenges to implement the NNsight. Could you elaborate more on such challenges and illustrate how you address them? And how the NNsight is different from the Torch Profiler and torch.jit.trace?\n",
    "For the practical usage like shown in Figure 4, I do not see obvious advantages of NNsight than using hooks of Torch. The required number of code lines of them are similar. Could you also elaborate more about this?\n",
    "[1] Theano: A python framework for fast computation of mathematical expressions. 2016 [2] PyTorch Profiler. https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html. [3] Torch.jit.trace. https://pytorch.org/docs/stable/generated/torch.jit.trace.html.\n",
    "\n",
    "Questions:\n",
    "Please refer to the weaknesses and provide more clarifications.\n",
    "\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 8: accept, good paper\n",
    "Confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "old_prompt = f\"\"\"\n",
    "# Write a review for this paper. The output should be in EXACTLY the same format as the following reviews:\n",
    "\n",
    "# Reviewer 1: \\n{reviewer_1}\n",
    "# Reviewer 2: \\n{reviewer_2}\n",
    "# Reviewer 3: \\n{reviewer_3}\n",
    "# Reviewer 4: \\n{reviewer_4}\n",
    "\n",
    "So, your review should follow the following format, in exactly the same way as the reviews above:\n",
    "\n",
    "Day Month Year, HH:MM (modified: Day Month Year, HH:MM)\n",
    "Summary:\n",
    "Soundness:\n",
    "Presentation:\n",
    "Contribution:\n",
    "Strengths:\n",
    "Weaknesses:\n",
    "Questions:\n",
    "Flag For Ethics Review:\n",
    "Rating:\n",
    "Confidence:\n",
    "Code Of Conduct:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for NDIF paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from anthropic import RateLimitError\n",
    "import time\n",
    "\n",
    "# get API key\n",
    "load_dotenv()\n",
    "api_key = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "prompt = \"\"\"\n",
    "# Write a review for this paper, using the ICLR reviewer guide as context. The review should output a text file with the following format EXACTLY, but without the ``` tags:\n",
    "\n",
    "```\n",
    "<Day Month Year, HH:MM (modified: Day Month Year, HH:MM)>\n",
    "Summary: <a paragraph>\n",
    "Soundness: <a number between 0 and 4>\n",
    "Presentation: <a number between 0 and 4>\n",
    "Contribution: <a number between 0 and 4>\n",
    "Strengths: <a paragraph>\n",
    "Weaknesses: <a paragraph>\n",
    "Questions: <a paragraph>\n",
    "Flag For Ethics Review:\n",
    "Rating: <overall rating, a number in [1, 3, 5, 6, 8, 10].>\n",
    "Confidence: <a number between 1 and 5>\n",
    "Code Of Conduct: <yes or no>\n",
    "```\n",
    "\n",
    "The possible values for overall rating are below:\n",
    "\n",
    "score\trating\n",
    "1\tstrong reject\n",
    "3\treject, not good enough\n",
    "5\tmarginally below the acceptance threshold\n",
    "6\tmarginally above the acceptance threshold\n",
    "8\taccept, good paper\n",
    "10\tstrong accept, should be highlighted at the conference\n",
    "\n",
    "DO NOT include any other text in your response.\n",
    "\"\"\"\n",
    "reviewer_guide = Path(\"context/iclr_reviewer_guide.md\").read_text()\n",
    "\n",
    "\n",
    "def count_tokens(text: str):\n",
    "    response = client.messages.count_tokens(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        system=\"You are an ICLR (International Conference on Learning Representations) reviewer\",\n",
    "        messages=[{\"role\": \"user\", \"content\": text}],\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "# the main function for getting a review\n",
    "def get_completion(url: str):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            message = client.messages.create(\n",
    "                model=\"claude-3-7-sonnet-20250219\",\n",
    "                system=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"You are an ICLR (International Conference on Learning Representations) reviewer\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": reviewer_guide,\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"},\n",
    "                    },\n",
    "                ],\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"document\",\n",
    "                                \"source\": {\n",
    "                                    \"type\": \"url\",\n",
    "                                    \"url\": url,\n",
    "                                },\n",
    "                            },\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            return message, message.content[0].text\n",
    "        except RateLimitError as e:\n",
    "            attempt += 1\n",
    "            backoff = min(60, 2**attempt)\n",
    "            print(\n",
    "                f\"rate limit hit, waiting {backoff:.2f} seconds before retry (attempt {attempt})\"\n",
    "            )\n",
    "            time.sleep(backoff)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewer_1: MessageTokensCount(input_tokens=617)\n",
      "reviewer_2: MessageTokensCount(input_tokens=482)\n",
      "reviewer_3: MessageTokensCount(input_tokens=531)\n",
      "reviewer_4: MessageTokensCount(input_tokens=758)\n"
     ]
    }
   ],
   "source": [
    "# token count in real reviews - use to set max_tokens above (1024 should be fine, based on this)\n",
    "for name, review_real in {\n",
    "    \"reviewer_1\": reviewer_1,\n",
    "    \"reviewer_2\": reviewer_2,\n",
    "    \"reviewer_3\": reviewer_3,\n",
    "    \"reviewer_4\": reviewer_4,\n",
    "}.items():\n",
    "    print(f\"{name}: {count_tokens(review_real)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://openreview.net/notes/edits/attachment?id=cxva7Lw6zB&name=pdf\"\n",
    "review, review_text = get_completion(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 October 2024, 14:32 (modified: 4 October 2024, 14:32)\n",
      "Summary: This paper introduces NNsight and NDIF, two technologies designed to democratize access to the internal representations and computations of large neural networks, particularly very large language models (LLMs). NNsight extends PyTorch to enable deferred remote execution through an \"intervention graph\" architecture that separates experiment design from model runtime, allowing researchers to define custom interventions using familiar PyTorch syntax. NDIF is a complementary cloud service that executes these intervention graphs, enabling shared access to pretrained models across multiple users. The authors present benchmarks comparing their approach to existing solutions and demonstrate how their framework addresses the growing gap in interpretability research on large-scale models.\n",
      "Soundness: 3\n",
      "Presentation: 3\n",
      "Contribution: 4\n",
      "Strengths: The paper addresses a significant practical challenge in AI research: the growing gap between state-of-the-art models and those that researchers can feasibly study. The intervention graph architecture is well-motivated and elegantly designed, offering a clean solution for decoupling experiment code from model execution. The authors provide extensive code examples and performance benchmarks that demonstrate the system's capabilities and advantages over traditional approaches. The NDIF service's co-tenancy model represents a significant advancement for democratizing access to large model internals by amortizing computational costs across multiple users. The paper includes a quantitative survey of interpretability research that effectively highlights the problem being addressed.\n",
      "Weaknesses: The paper could benefit from more detailed discussion of security and safety considerations in the NDIF service, especially regarding potential misuse or vulnerabilities when multiple users share access to model parameters. The authors mention a \"server-side whitelist of permitted operations\" but don't elaborate on how this is implemented or maintained. While the performance benchmarks are helpful, they focus primarily on activation patching but don't evaluate more complex intervention scenarios that might stress the system differently. Some technical details about how the intervention graph is optimized before execution are lacking, which would be valuable for understanding the system's scalability to more complex experiments. The paper doesn't thoroughly address how model versioning is handled in NDIF, which could be important for reproducible research.\n",
      "Questions: How does NDIF handle versioning of models to ensure reproducibility of experiments over time? What specific security measures are implemented to prevent malicious users from extracting model weights or compromising other users' experiments? Have you considered extending NNsight beyond PyTorch to other frameworks like JAX or TensorFlow? How does the framework handle very large interventions that might introduce significant computational overhead? Does NDIF support any form of experiment caching to improve performance for repeated executions of similar experiments?\n",
      "Flag For Ethics Review: \n",
      "Rating: 8\n",
      "Confidence: 4\n",
      "Code Of Conduct: yes\n"
     ]
    }
   ],
   "source": [
    "print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessageTokensCount(input_tokens=643)\n"
     ]
    }
   ],
   "source": [
    "print(count_tokens(review_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "openreview_url=HttpUrl('https://openreview.net/forum?id=5sRnsubyAK') pdf_url=HttpUrl('https://openreview.net/pdf?id=5sRnsubyAK') submission_date=datetime.datetime(2024, 9, 28, 0, 0)\n",
      "https://openreview.net/forum?id=5sRnsubyAK\n",
      "https://openreview.net/pdf?id=5sRnsubyAK\n",
      "2024-09-28 00:00:00\n",
      "5sRnsubyAK\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from models.schemas import PaperMetadata\n",
    "import json\n",
    "\n",
    "\n",
    "data_folder = Path(\"data\")\n",
    "\n",
    "paper_folder = list(data_folder.iterdir())[3]\n",
    "\n",
    "metadata = PaperMetadata.from_json(paper_folder / \"metadata.json\")\n",
    "print(metadata)\n",
    "print(metadata.openreview_url)\n",
    "print(metadata.pdf_url)\n",
    "print(metadata.submission_date)\n",
    "print(metadata.paper_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review for odjMSBSWRt already exists, skipping...\n",
      "Review for hWF0HH8Rr9 already exists, skipping...\n",
      "Reviewing Ql7msQBqoF...\n",
      "Saved review for Ql7msQBqoF to data/Ql7msQBqoF/Ql7msQBqoF_claude_base_review.txt \n",
      "\n",
      "Reviewing 5sRnsubyAK...\n",
      "rate limit hit, waiting 2.00 seconds before retry (attempt 1)\n",
      "rate limit hit, waiting 4.00 seconds before retry (attempt 2)\n",
      "rate limit hit, waiting 8.00 seconds before retry (attempt 3)\n",
      "rate limit hit, waiting 16.00 seconds before retry (attempt 4)\n",
      "Saved review for 5sRnsubyAK to data/5sRnsubyAK/5sRnsubyAK_claude_base_review.txt \n",
      "\n",
      "Reviewing viQ1bLqKY0...\n",
      "rate limit hit, waiting 2.00 seconds before retry (attempt 1)\n",
      "rate limit hit, waiting 4.00 seconds before retry (attempt 2)\n",
      "Saved review for viQ1bLqKY0 to data/viQ1bLqKY0/viQ1bLqKY0_claude_base_review.txt \n",
      "\n",
      "Reviewing llW4qRsF0o...\n",
      "rate limit hit, waiting 2.00 seconds before retry (attempt 1)\n",
      "rate limit hit, waiting 4.00 seconds before retry (attempt 2)\n",
      "rate limit hit, waiting 8.00 seconds before retry (attempt 3)\n",
      "rate limit hit, waiting 16.00 seconds before retry (attempt 4)\n",
      "rate limit hit, waiting 32.00 seconds before retry (attempt 5)\n"
     ]
    }
   ],
   "source": [
    "# generate claude reviews for everything in data folder\n",
    "\n",
    "reviews = []\n",
    "messages = []\n",
    "for paper_folder in data_folder.iterdir():\n",
    "    metadata = PaperMetadata.from_json(paper_folder / \"metadata.json\")\n",
    "    review_path = paper_folder / f\"{metadata.paper_id}_claude_base_review.txt\"\n",
    "\n",
    "    # Check if review already exists (caching)\n",
    "    if review_path.exists():\n",
    "        print(f\"Review for {metadata.paper_id} already exists, skipping...\")\n",
    "        # Optionally load existing review into the reviews list\n",
    "        review_text = review_path.read_text()\n",
    "        reviews.append(review_text)\n",
    "        continue\n",
    "\n",
    "    print(f\"Reviewing {metadata.paper_id}...\")\n",
    "    message, review = get_completion(str(metadata.pdf_url))\n",
    "    reviews.append(review)\n",
    "    messages.append(message)\n",
    "    review_path.write_text(review)\n",
    "    print(f\"Saved review for {metadata.paper_id} to {review_path}\", \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
