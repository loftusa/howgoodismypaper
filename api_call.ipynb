{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndif reviewer scores\n",
    "reviewer_1 = \"\"\"\n",
    "Nov 2024, 12:56 (modified: 02 Dec 2024, 10:19)\n",
    "Summary:\n",
    "The paper introduces two systems NNsight and NDIF that collectively aim to reduce the developer and hardware costs of analyzing and modifying the inference behavior of open-source models. NNsight is an instrumentation framework for PyTorch models, while NDIF is an inference service that enables deferred execution of NNsight instrumentations on a remote shared model deployment. The paper provides evaluation comparing to inference baselines of shared (Petal) and non-shared HPC deployments.\n",
    "\n",
    "Soundness: 3: good\n",
    "Presentation: 3: good\n",
    "Contribution: 3: good\n",
    "Strengths:\n",
    "This paper is focusing on an important problem since tools that enable introspection of model internals are extremely valuable for ML research and applications.\n",
    "Opting for Pytorch-native tools is a great design choice as that significantly reduces development and integration burden for practitioners.\n",
    "Enabling resource sharing via NDIF service is very useful to democratizing access to SOTA models.\n",
    "Weaknesses:\n",
    "The paper seems to overclaim, particularly in the title and abstract, that the work applies to foundation models in general, when in reality it only applies to open-source models, since model internals knowledge is required to create interventions. The authors should more carefully scope the claims.\n",
    "The evaluation does not study co-tenancy scenarios where the NDIF is servicing multiple NNsight requests. This is a significant oversight considering that the resource sharing benefits of NDIF is a major contribution of the work. The authors should include results showing not just the performance implications of servicing the multiple NNsight requests but also validating the correctness of the co-tenancy features.\n",
    "Questions:\n",
    "Do users submit a pair of NNsight request and an input prompt? Or how is the input prompt for exercising an intervention generated?\n",
    "When multiple NNsight requests are submitted, are all the interventions applied to a single model instance for inference, or is a separate model instance created for each request?\n",
    "If a single model instance is instrumented with multiple NNsight requests, how does NDIF ensure that a request that modifies model parameters does not affect other requests?\n",
    "How is KV-cache managed for multiple NNsight requests?\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 6: marginally above the acceptance threshold\n",
    "Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "reviewer_2 = \"\"\"\n",
    "Nov 2024, 06:59 (modified: 29 Nov 2024, 00:57)\n",
    "Summary:\n",
    "This paper introduces NNsight and NDIF, two open systems that provide efficient, transparent access to the internals of large neural networks for research purposes. NNsight extends PyTorch to offer deferred remote execution of intervention graphs, while NDIF serves as a scalable inference service that executes these intervention requests, enabling resource sharing among users. The work addresses challenges like limited access to state-of-the-art models and the significant resource demands of large-scale AI research by sharing resources.\n",
    "\n",
    "Soundness: 2: fair\n",
    "Presentation: 3: good\n",
    "Contribution: 3: good\n",
    "Strengths:\n",
    "The intervention graph extends the model computational graph and decouples experimental design from model runtime, reducing engineering complexity.\n",
    "NDIF effectively shares GPU resources among researchers (co-tenancy), reducing cost and enabling large-scale experiments\n",
    "This is a novel idea with great potential benefit to the research community.\n",
    "Weaknesses:\n",
    "The evaluation section is limited to the end-to-end performance with little detail on the system optimizations. Specifically, more information is needed to assess the performance and scalability of this system using an increasing number of users.\n",
    "While NDIF addresses large-scale experiments on open models, it does not cover closed, proprietary models hosted proprietarily.\n",
    "Lack of discussion with relevant co-serving systems like S-LoRA (Sheng et al, MLSys 2024) and dLoRA (Wu et al, OSDI 2024).\n",
    "Questions:\n",
    "Could you please share more evaluation results on the performance and scalability of the proposed systems?\n",
    "\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 6: marginally above the acceptance threshold\n",
    "Confidence: 3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "reviewer_3 = \"\"\"\n",
    "03 Nov 2024, 09:04 (modified: 12 Nov 2024, 11:31)\n",
    "Summary:\n",
    "This work presents a new PyTorch-compatible library for remotely accessing the internal structures of deep-neural-network-based models such as large language models (LLMs) and altering the way they operate. Using this library, it is possible to produce intervention code which is hooked into the original model to read or replace original model parameters and activations. This intervention code is translated into a directed acyclic intervention graph that augments the computational graph of the original model. Others have proposed similar, but less flexible mechanisms. For instance, pyvene (Wu et al., 2024) supports dictionary-based intervention definitions instead of the code-based interventions proposed by this work, which can also be transformed into graph-based representations and further optimised, e.g., by TorchScript. Petals (Borzunov et al., 2023), on the other hand, does not support remote execution of the intervention code, requiring it to be executed locally, which limits virtualisation opportunities and leads to additional communication overheads.\n",
    "\n",
    "Soundness: 3: good\n",
    "Presentation: 3: good\n",
    "Contribution: 2: fair\n",
    "Strengths:\n",
    "The proposed approach involves some new mechanisms that complement the related work.\n",
    "By enabling remote execution of the intervention code the authors have demonstrated a superior performance compared to Petals.\n",
    "Weaknesses:\n",
    "An interesting real-life application of the infrastructure built is missing. For instance, the authors could consider demonstrating their system on a large-scale interpretability study or model editing task that would be infeasible without their infrastructure.\n",
    "\n",
    "Questions:\n",
    "It looks like the main technical contribution of this work is the enablement of efficient/virtualised remote interventions on LLMs. Could the authors elaborate on why they believe this contribution is relevant and impactful for the ICLR community?\n",
    "\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 6: marginally above the acceptance threshold\n",
    "Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "reviewer_4 = \"\"\"\n",
    "31 Oct 2024, 11:07 (modified: 28 Nov 2024, 00:19)\n",
    "Summary:\n",
    "The paper introduces NNsight and NDIF, a framework and a cloud inference service designed to facilitate research on large-scale AI models, and enable study of the representations and computations learned by large neural networks. This work proposes a method for organizing experiments on very large models, reducing engineering burden, enhancing reproducibility, and enabling low-cost communication with remote models. The NNsight is an open-source implementation of the intervention graph architecture that extends PyTorch to support transparent model interventions without requiring local storage or management of model parameters. And the NDIF is an open-source cloud inference service that supports the NNsight by providing behind-the-scenes user sharing of model instances to reduce the costs of large-scale AI research.\n",
    "\n",
    "Soundness: 3: good\n",
    "Presentation: 3: good\n",
    "Contribution: 4: excellent\n",
    "Strengths:\n",
    "The intervention graph architecture and the NNsight system provide a practical and efficient way to decouple experimental and engineering code, making it easier to conduct complex experiments on studying the intermediate representations and gradients of large models.\n",
    "The paper includes a thorough performance evaluation comparing NNsight and NDIF with HPC and Petals, demonstrating the efficiency and effectiveness of the proposed solutions.\n",
    "The performance comparisons between NNsight and other frameworks like baukit, pyvene, TransformerLens show the similar efficiency of NNsight.\n",
    "The remote execution of NNIF provides the easy usage for users.\n",
    "Weaknesses:\n",
    "While the developed tools of NNsight and NDIF are very useful in scientific study of AI models, there is no founded insights of this work. I'm not sure whether this contribution is enough for the acceptance.\n",
    "Exploiting the DAG representation is not novel. As indicated by the authors, [1] has proposed the DAG concepts and use it to accelerate computation. Besides, the Torch Profiler [2] and torch.jit.trace [3] can help to dissect the model. I do not see any obvious technical challenges to implement the NNsight. Could you elaborate more on such challenges and illustrate how you address them? And how the NNsight is different from the Torch Profiler and torch.jit.trace?\n",
    "For the practical usage like shown in Figure 4, I do not see obvious advantages of NNsight than using hooks of Torch. The required number of code lines of them are similar. Could you also elaborate more about this?\n",
    "[1] Theano: A python framework for fast computation of mathematical expressions. 2016 [2] PyTorch Profiler. https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html. [3] Torch.jit.trace. https://pytorch.org/docs/stable/generated/torch.jit.trace.html.\n",
    "\n",
    "Questions:\n",
    "Please refer to the weaknesses and provide more clarifications.\n",
    "\n",
    "Flag For Ethics Review: No ethics review needed.\n",
    "Rating: 8: accept, good paper\n",
    "Confidence: 5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.\n",
    "Code Of Conduct: Yes\n",
    "\"\"\"\n",
    "\n",
    "old_prompt = f\"\"\"\n",
    "# Write a review for this paper. The output should be in EXACTLY the same format as the following reviews:\n",
    "\n",
    "# Reviewer 1: \\n{reviewer_1}\n",
    "# Reviewer 2: \\n{reviewer_2}\n",
    "# Reviewer 3: \\n{reviewer_3}\n",
    "# Reviewer 4: \\n{reviewer_4}\n",
    "\n",
    "So, your review should follow the following format, in exactly the same way as the reviews above:\n",
    "\n",
    "```\n",
    "Day Month Year, HH:MM (modified: Day Month Year, HH:MM)\n",
    "Summary:\n",
    "Soundness:\n",
    "Presentation:\n",
    "Contribution:\n",
    "Strengths:\n",
    "Weaknesses:\n",
    "Questions:\n",
    "Flag For Ethics Review:\n",
    "Rating:\n",
    "Confidence:\n",
    "Code Of Conduct:\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for NDIF paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get API key\n",
    "load_dotenv()\n",
    "api_key = os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "prompt = \"\"\"\n",
    "# Write a review for this paper, using the ICLR reviewer guide as context. The review should follow the following format EXACTLY:\n",
    "\n",
    "```\n",
    "Day Month Year, HH:MM (modified: Day Month Year, HH:MM)\n",
    "Summary: <a paragraph>\n",
    "Soundness: <a number between 0 and 4>\n",
    "Presentation: <a number between 0 and 4>\n",
    "Contribution: <a number between 0 and 4>\n",
    "Strengths: <a paragraph>\n",
    "Weaknesses: <a paragraph>\n",
    "Questions: <a paragraph>\n",
    "Flag For Ethics Review:\n",
    "Rating: <overall rating, a number in [1, 3, 5, 6, 8, 10].>\n",
    "Confidence: <a number between 1 and 5>\n",
    "Code Of Conduct: <yes or no>\n",
    "```\n",
    "\n",
    "The possible values for overall rating are below:\n",
    "\n",
    "score\trating\n",
    "1\tstrong reject\n",
    "3\treject, not good enough\n",
    "5\tmarginally below the acceptance threshold\n",
    "6\tmarginally above the acceptance threshold\n",
    "8\taccept, good paper\n",
    "10\tstrong accept, should be highlighted at the conference\n",
    "\"\"\"\n",
    "reviewer_guide = Path(\"context/iclr_reviewer_guide.md\").read_text()\n",
    "\n",
    "\n",
    "def get_completion(url: str):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        system=\"You are an ICLR (International Conference on Learning Representations) reviewer\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": reviewer_guide,\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"document\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"url\",\n",
    "                            \"url\": url,\n",
    "                        },\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return message, message.content[0].text\n",
    "\n",
    "\n",
    "url = \"https://openreview.net/notes/edits/attachment?id=cxva7Lw6zB&name=pdf\"\n",
    "message, review = get_completion(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "26 October 2023, 15:30 (modified: 27 October 2023, 09:15)\n",
      "Summary: This paper presents NNsight and NDIF, a set of tools to democratize access to internal representations of large neural networks. NNsight is an open-source Python library extending PyTorch that provides a simple API for creating \"intervention graphs\" that specify operations to access and manipulate neural network internals. NDIF is a scalable inference service that executes these intervention graphs on shared, large-scale model deployments. The authors motivate their work by showing a growing gap between the models used in interpretability research and state-of-the-art models. They evaluate the performance of their approach against traditional high-performance computing and other frameworks like Petals, demonstrating that NNsight and NDIF effectively reduce barriers to studying large models.\n",
      "\n",
      "Soundness: 3\n",
      "Presentation: 4\n",
      "Contribution: 4\n",
      "Strengths: \n",
      "The paper addresses a significant problem in the AI research community - the growing gap between state-of-the-art models and the models that are being studied for interpretability. The intervention graph abstraction is well-designed and elegantly separates experimental code from model deployment details. The system makes impressive strides toward democratizing access to large model internals. The empirical evaluation thoroughly compares the approach against alternatives, showing clear advantages particularly for larger models. The presentation is clear, and the examples illustrate how the framework simplifies common interpretability research tasks.\n",
      "\n",
      "Weaknesses: \n",
      "While the paper demonstrates performance benefits compared to existing approaches, some details about the security and isolation model for NDIF could be elaborated further. The paper doesn't fully address potential competition between multiple intervention graphs running simultaneously on the same model and how resource allocation is managed in these cases. The system seems primarily focused on transformer architectures, though the principles should generalize; more discussion of applicability to other architectures would strengthen the work.\n",
      "\n",
      "Questions: \n",
      "1. How does NDIF handle potential resource contention when multiple users run computationally expensive interventions on the same model concurrently?\n",
      "2. What happens when intervention graphs contain incompatible operations (e.g., one user tries to modify activations that another user's intervention depends on)?\n",
      "3. Does the approach scale to multi-modal models, or are there specific challenges that arise in those contexts?\n",
      "4. How are the security boundaries enforced in NDIF to ensure one user cannot inappropriately access another user's data or interventions?\n",
      "\n",
      "Flag For Ethics Review: No\n",
      "Rating: 8\n",
      "Confidence: 4\n",
      "Code Of Conduct: Yes\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openreview_url': 'https://openreview.net/forum?id=odjMSBSWRt',\n",
       " 'pdf_url': 'https://openreview.net/notes/edits/attachment?id=75k8x8fSoP&name=pdf',\n",
       " 'submission_date': '2024-09-28T00:00:00'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.schemas import PaperMetadata\n",
    "import json\n",
    "\n",
    "\n",
    "data_folder = Path(\"data\")\n",
    "\n",
    "paper_folder = next(data_folder.iterdir())\n",
    "metadata = (paper_folder / \"metadata.json\").read_text()\n",
    "metadata = PaperMetadata.model_validate_json(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/odjMSBSWRt')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_folder.iterdir())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
